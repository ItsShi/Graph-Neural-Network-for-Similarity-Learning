{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItsShi/GraphNeuralNetworkforSimilarityLearning/blob/main/QM7b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcFDPMRlHHoy"
      },
      "source": [
        "# Graph Matching Networks for the Similarity Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ0w7YyKHAoF"
      },
      "source": [
        "## Some dependencies and imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_4INgQJ_Odc"
      },
      "source": [
        "These are all the dependencies that will be used in this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEIxulV5XNuy",
        "outputId": "9a35bbe6-2b14-4751-b595-1e4a9fb6e38c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Mar 30 10:05:14 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrM8LiF4XbaS",
        "outputId": "7ed9a7a7-c744-4dac-bbd0-03ce9d353029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErPy3iq51gPF",
        "outputId": "9dc7f447-279e-4d32-d05e-06b34521b5fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 1.10.0+cu111\n",
            "Uninstalling torch-1.10.0+cu111:\n",
            "  Would remove:\n",
            "    /usr/local/bin/convert-caffe2-to-onnx\n",
            "    /usr/local/bin/convert-onnx-to-caffe2\n",
            "    /usr/local/bin/torchrun\n",
            "    /usr/local/lib/python3.7/dist-packages/caffe2/*\n",
            "    /usr/local/lib/python3.7/dist-packages/torch-1.10.0+cu111.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled torch-1.10.0+cu111\n",
            "Collecting torch==1.5\n",
            "  Downloading torch-1.5.0-cp37-cp37m-manylinux1_x86_64.whl (752.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 752.0 MB 9.2 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5) (1.21.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5) (0.16.0)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.5.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.5.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.5.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.5.0\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n",
            "\n",
            "\n",
            "1.5.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip uninstall torch  #1.10.0+cu111 会报错\n",
        "!pip install torch==1.5 #1.3或者1.2\n",
        "!nvcc --version\n",
        "print(\"\\n\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "print(torch.__version__)\n",
        "\n",
        "# !pip install torch-scatter \n",
        "# !pip install torch-sparse \n",
        "# !pip install torch-geometric\n",
        "# import torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phaxfe70Tonf",
        "outputId": "7bba04ff-7797-4447-a87c-3927626c8df8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting dgl-cu101\n",
            "  Downloading dgl_cu101-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (36.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 36.2 MB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu101) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl-cu101) (2.6.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu101) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu101) (1.21.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu101) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu101) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu101) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu101) (2.10)\n",
            "Installing collected packages: dgl-cu101\n",
            "Successfully installed dgl-cu101-0.6.1\n"
          ]
        }
      ],
      "source": [
        "#dgl\n",
        "# !git init\n",
        "# !git clone https://github.com/dmlc/dgl.git\n",
        "# !git submodule init\n",
        "# !git submodule update\n",
        "\n",
        "!pip install dgl-cu101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a60CR_iny2KU"
      },
      "outputs": [],
      "source": [
        "# #gpu\n",
        "# if torch.cuda.is_available():\n",
        "#   !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\n",
        "#   !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.9.0+cu111.html #gpu \n",
        "#   !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "#   import torch_geometric\n",
        "\n",
        "# else:\n",
        "#   !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cpu.html\n",
        "#   !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cpu.html  #cpu pytorch1.10.0\n",
        "#   !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "#   import torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcXQ-x6hoJXd"
      },
      "outputs": [],
      "source": [
        "#pascal VOC dataset\n",
        "# !pip install opencv-python\n",
        "# !pip3 install torchvision\n",
        "\n",
        "# print(\"下载github/..zip：\")\n",
        "# !pip install https://github.com/fastai/fastai/archive/master.zip\n",
        "\n",
        "# print(\"更新和安装: -h（帮助），-y（当安装过程提示选择全部为yes），-q（不显示安装的过程）\")\n",
        "# !apt update && apt install -y libsm6 libxext6\n",
        "\n",
        "# print(\"下载pytorch/..whl: \")\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "\n",
        "# !mkdir data  \n",
        "\n",
        "# #Wget： supports downloading via HTTP, HTTPS, and FTP\n",
        "# !wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar -P data/ \n",
        "\n",
        "# print(\"\\n\")\n",
        "# !wget https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip -P data/\n",
        "\n",
        "# #tar(tarball): for collecting many files into one archive file, \"tape archive\"\n",
        "# print(\"归档.tar：\")\n",
        "# !tar -xf data/VOCtrainval_06-Nov-2007.tar -C data/\n",
        "\n",
        "# print(\"解压zip：\")\n",
        "# !unzip data/PASCAL_VOC.zip -d data/\n",
        "\n",
        "# print(\"remove文件：\")\n",
        "# !rm -rf data/PASCAL_VOC.zip data/VOCtrainval_06-Nov-2007.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "npXw6F8LpTCx",
        "outputId": "c7d6554b-1f32-4a3f-b44e-d65355d06502"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#example\\nfrom pygmtools.benchmark import Benchmark\\n\\n# Define Benchmark on PascalVOC.\\nbm = Benchmark(name='PascalVOC', sets='train', \\n               obj_resize=(256, 256), problem='2GM',\\n               filter='intersection')\\n\\n# Random fetch data and ground truth.\\ndata_list, gt_dict, _ = bm.rand_get_data(cls=None, num=2)\\n\""
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#compare existing deep graph matching algorithms under different datasets\n",
        "#provides a unified data interface and an evaluating platform \n",
        "#pygmtools supports 5 datasets, including PascalVOC, Willow-Object, SPair-71k, CUB2011 and IMC-PT-SparseGM.\n",
        "# !pip uninstall Pillow\n",
        "# !pip install Pillow==7.2.0\n",
        "# !pip install pygmtools\n",
        "#dataset.py: to download dataset and process the dataset into a json file, and also save train set and test set.\n",
        "#benchmark.py:  to fetch data from json file and evaluate prediction result.\n",
        "#dataset_config.py: Fixed dataset settings, mostly dataset path and classes.\n",
        "\n",
        "'''\n",
        "evaluation metrics include matching_precision (p), matching_recall (r) and f1_score (f1). \n",
        "Also, to measure the reliability of the evaluation result, \n",
        "we define coverage (cvg) for each class in the dataset as number of evaluated pairs in the class / number of all possible pairs in the class. \n",
        "Therefore, larger coverage refers to higher reliability.\n",
        "'''\n",
        "\n",
        "'''\n",
        "#example\n",
        "from pygmtools.benchmark import Benchmark\n",
        "\n",
        "# Define Benchmark on PascalVOC.\n",
        "bm = Benchmark(name='PascalVOC', sets='train', \n",
        "               obj_resize=(256, 256), problem='2GM',\n",
        "               filter='intersection')\n",
        "\n",
        "# Random fetch data and ground truth.\n",
        "data_list, gt_dict, _ = bm.rand_get_data(cls=None, num=2)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wneTI5RJt9U-"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "1kuxKzWQXKoR",
        "outputId": "712ec5ad-3958-48a8-a1a3-9a7b8177013c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport matplotlib\\nimport networkx\\nimport numpy\\nimport pandas\\n# import scikitlearn\\nimport scipy\\nimport texttable\\nimport tqdm\\nimport Cython\\n\\nprint(matplotlib. __version__)\\nprint(networkx. __version__)\\nprint(numpy. __version__)\\nprint(pandas. __version__)\\nprint(matplotlib. __version__)\\nprint(scipy. __version__)\\nprint(texttable. __version__)\\nprint(tqdm. __version__)\\nprint(Cython. __version__)\\n\\n# matplotlib==3.3.4\\n# networkx==2.5\\n# numpy==1.20.1\\n# pandas==1.2.3\\n# scikit-learn==0.24.1\\n# scipy==1.6.1\\n# texttable==1.6.3\\n# tqdm==4.59.0\\n# Cython==0.29.23\\n\\n'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "import matplotlib\n",
        "import networkx\n",
        "import numpy\n",
        "import pandas\n",
        "# import scikitlearn\n",
        "import scipy\n",
        "import texttable\n",
        "import tqdm\n",
        "import Cython\n",
        "\n",
        "print(matplotlib. __version__)\n",
        "print(networkx. __version__)\n",
        "print(numpy. __version__)\n",
        "print(pandas. __version__)\n",
        "print(matplotlib. __version__)\n",
        "print(scipy. __version__)\n",
        "print(texttable. __version__)\n",
        "print(tqdm. __version__)\n",
        "print(Cython. __version__)\n",
        "\n",
        "# matplotlib==3.3.4\n",
        "# networkx==2.5\n",
        "# numpy==1.20.1\n",
        "# pandas==1.2.3\n",
        "# scikit-learn==0.24.1\n",
        "# scipy==1.6.1\n",
        "# texttable==1.6.3\n",
        "# tqdm==4.59.0\n",
        "# Cython==0.29.23\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXU4exQlJt58"
      },
      "outputs": [],
      "source": [
        "#%loadpy '-/Users/xiaoweishi/Downloads/Graph-Matching-Networks-main/segment.py'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_-u89JgXg7N"
      },
      "source": [
        "交叉图注意力的匹配机制对输入的一对图进行联合推理，计算它们之间的相似度得分\n",
        "\n",
        "#Background：\n",
        "现有的GNN通过迭代聚合局部结构信息的传播过程设计和计算图节点表示，对图元素的排列保持不变，然后将这些节点表示直接用于节点分类，或汇集到图向量中进行图分类。\n",
        "GNN 对监督分类或回归之外的问题的研究相对较少。\n",
        "\n",
        "#主要任务：\n",
        "1 证明GNN可以为相似性学习提供嵌入  2 设计GMN计算图间相似匹配\n",
        "\n",
        "相关工作：WL研究图同构（相同）与本文要研究图相似有区别，图核更重于设计，基于图论，没有基于学习，不端到端，Siamese网络可通过网格化计算相似度\n",
        "\n",
        "#标准GNN和本文的GMN：\n",
        "GEM：编码器-传播层-聚合（MLP构造节点和边的嵌入向量h和e-MLP或RNN将节点嵌入映射到新的节点表示 多层传播后每个节点的表示其局部邻域中积累信息-输入一系列节点表示计算出一个图表示 带有门控权重的聚合方法可以滤波掉无关信息，比求和等方法好，有了两个图的表示就可以在欧式空间计算相似度了，比如欧氏距离，余弦相似度，汉明相似度等如果没有传播层就是计算独立点的表示，再池化到整个图上，但这样就忽视了结构信息，单纯在处理一堆独立数据）\n",
        "GMN：不单独嵌入每个图表示，联合计算相似度，改变每个传播层的节点更新模块，不仅考虑边上的聚合信息，而且考虑图间匹配向量，用来衡量一个图中的节点与另一个图中的一个或多个节点的匹配程度\n",
        "GMN花时间，注意力模块有两个好处：两图完美匹配时，注意力权重达到峰值，μ的和为0，这样两图在下一轮传播中会计算相同的表示图间差异会记录在μ的和中，通过传播逐渐放大，可以让匹配模型对差异更加敏感。\n",
        "匹配模型可以基于其他图来改变当前图的表示，也即调整图的表示使得图间差异愈发明显便于匹配\n",
        "\n",
        "#训练学习：\n",
        "可以选择成对训练，还是三元训练\n",
        "成对训练需要标签positive (相似) or negative (不相似)\n",
        "而三元训练只需要相对相似，即 G1 更接近 G2 还是 G3\n",
        "\n",
        "使用欧式相似度，基于边缘的成对损失\n",
        "\n",
        "$$\n",
        "L_\\mathrm{pair} = \\mathbb{E}_{(G_1, G_2, t)}[\\max\\{0, \\gamma - t(1 - d(G_1, G_2))\\}]\n",
        "$$\n",
        "\n",
        "\n",
        "其中t ∈ {−1, 1}作为这一对图的标签，γ > 0是边缘参数，d是欧氏距离\n",
        "当这一对相似（t=1）时，损失L收敛到0会让d< 1−γ；当 t = −1 时，d> 1+γ。\n",
        "\n",
        "给定 G1 和 G2 比 G1 和 G3 更接近的三元组，优化以下基于边距的三元组损失：\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "损失收敛到0可以通过边缘γ让d(G1, G2)比d(G1, G3) 小\n",
        "\n",
        "将图表示成二进制-1或1，可以最小化相似对的汉明距离，最大化不相似对的距离（扩大类间距，缩小类内距的方法吗？）这样可以提高检索效率\n",
        "于是可以引入tanh变化优化损失函数\n",
        "\n",
        "$$\n",
        "s(G_1, G_2) = \\frac{1}{H}\\sum_{i=1}^H \\tanh(h_{G_1, i}) \\cdot \\tanh(h_{G_2, i}),\n",
        "$$\n",
        "\n",
        "$$\n",
        "L_\\mathrm{pair} = \\mathbb{E}_{(G_1, G_2, t)}[(t - s(G_1, G_2))^2] / 4.\n",
        "$$\n",
        "\n",
        "其中\n",
        "是近似的平均汉明距离（有点像多图递归矩阵补全代码里的那个损失函数处理）这样会比基于边缘的损失更稳定\n",
        "\n",
        "\n",
        "\n",
        "#实验 训练过程：：\n",
        "GED被定义为图间最小距离，因为是NP难所以近似（但是既然近似计算相似度的这些算法里没有用到GED或者其思想，那为啥还要说它呢，不就和它没关系吗？）\n",
        "\n",
        "\n",
        "随机构造一个n节点，边概率p的图G1，然后再随机替换kp条边为新边构造相似样本G2，替换kn条边构造不相似样本G3，kp<kn（原来如此，代码里构造数据集的过程类似于GED，这不是NP难吗，怪不得运行不出来？）模型需要预测相似对（G1，G2）的相似度高于不相似对（G1，G3）\n",
        "\n",
        "对于 GMN，可以将图间注意力可视化，以进一步了解它是如何工作的。当两个图匹配时，注意力权重可以很好地对齐节点，而当它们不匹配时，往往会关注度数较高的节点。 然而，该模式不像标准注意力模型那样具有可解释性。\n",
        "\n",
        "\n",
        "从相似性学习设置中学习的表示也可以轻松地泛化到训练期间未见过的类的数据（零样本泛化）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et0QYHr1_EZE"
      },
      "source": [
        "### Graph embedding model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MuLL3WXOS42"
      },
      "source": [
        "#### The graph encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrT0Cvk6tO9G"
      },
      "source": [
        "\n",
        "$$d(G_1, G_2) = d_H(embed(G_1), embed(G_2)),$$\n",
        "\n",
        "\n",
        "1. $embed$ maps any graph  $G$  into an $H$ -dimensional vector:\n",
        "\n",
        "$embed(G_1) = $\n",
        "\n",
        "1) mapping: \n",
        "\n",
        "$$\\begin{array}{rcl}\n",
        "h_i^{(0)} &=& \\mathrm{MLP_{node}}(x_i) \\\\\n",
        "e_{ij} &=& \\mathrm{MLP_{edge}}(x_{ij})\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "2) iterative message passing: \n",
        "\n",
        "$$\\begin{array}{rcl}\n",
        "m_{i\\rightarrow j} &=& f_\\mathrm{message}(h_i^{(t)}, h_j^{(t)}, e_{ij}) \\\\\n",
        "h_i^{(t+1)} &=& f_\\mathrm{node}(h_i^{(t)}, \\sum_{j:(j,i)\\in E} m_{j\\rightarrow i})\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "3) aggregate node representations to get graph representations\n",
        "\n",
        "\n",
        "$$h_G = \\mathrm{MLP_G}\\left(\\sum_{i\\in V} h_i^{(T)}\\right).$$\n",
        "$$h_G = \\mathrm{MLP_G}\\left(\\sum_{i\\in V} \\sigma(\\mathrm{MLP_{gate}}(h_i^{(T)})) \\odot \\mathrm{MLP}(h_i^{(T)})\\right).$$\n",
        "\n",
        "2. $d_H$ is an existing distance metric\\:\n",
        "\n",
        "Euclidean distance: $d_H(x, y) = \\sqrt{\\sum_{i=1}^H (x_i - y_i)^2}$,\n",
        "\n",
        "or Hamming distance: $d_H(x, y)=\\sum_{i=1}^H \\mathbb{I}[x_i \\ne y_i]$\n",
        "\n",
        "  \n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBfDbLUUGzlE"
      },
      "source": [
        "节点嵌入：\n",
        "\n",
        "FCCN 层，（Linear）. ReLU 作为激活函数。\n",
        "\n",
        "聚合层：\n",
        "\n",
        "聚合函数（GRU，FCCN）\n",
        "\n",
        "Adam优化器。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQQuFd0vF_zm"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class GraphEncoder(nn.Module):\n",
        "    \"\"\"Encoder module that projects node and edge features to some embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 node_feature_dim,\n",
        "                 edge_feature_dim,\n",
        "                 node_hidden_sizes=None,\n",
        "                 edge_hidden_sizes=None,\n",
        "                 name='graph-encoder'):\n",
        "      \n",
        "        super(GraphEncoder, self).__init__()\n",
        "\n",
        "        self._node_feature_dim = node_feature_dim\n",
        "        self._edge_feature_dim = edge_feature_dim\n",
        "        self._node_hidden_sizes = node_hidden_sizes if node_hidden_sizes else None\n",
        "        self._edge_hidden_sizes = edge_hidden_sizes\n",
        "        self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        layer = []\n",
        "        layer.append(nn.Linear(self._node_feature_dim, self._node_hidden_sizes[0]))\n",
        "        for i in range(1, len(self._node_hidden_sizes)):\n",
        "            layer.append(nn.ReLU())\n",
        "            layer.append(nn.Linear(self._node_hidden_sizes[i - 1], self._node_hidden_sizes[i]))\n",
        "        self.MLP1 = nn.Sequential(*layer)\n",
        "\n",
        "        if self._edge_hidden_sizes is not None:\n",
        "            layer = []\n",
        "            layer.append(nn.Linear(self._edge_feature_dim, self._edge_hidden_sizes[0]))\n",
        "            for i in range(1, len(self._edge_hidden_sizes)):\n",
        "                layer.append(nn.ReLU())\n",
        "                layer.append(nn.Linear(self._edge_hidden_sizes[i - 1], self._edge_hidden_sizes[i]))\n",
        "            self.MLP2 = nn.Sequential(*layer)\n",
        "        else:\n",
        "            self.MLP2 = None\n",
        "\n",
        "    def forward(self, node_features, edge_features=None):\n",
        "        \"\"\"Encode node and edge features.\n",
        "        Args:\n",
        "          node_features: [n_nodes, node_feat_dim] float tensor.\n",
        "          edge_features: if provided, should be [n_edges, edge_feat_dim] float\n",
        "            tensor.\n",
        "        Returns:\n",
        "          node_outputs: [n_nodes, node_embedding_dim] float tensor, node embeddings.\n",
        "          edge_outputs: if edge_features is not None and edge_hidden_sizes is not\n",
        "            None, this is [n_edges, edge_embedding_dim] float tensor, edge\n",
        "            embeddings; otherwise just the input edge_features.\n",
        "        \"\"\"\n",
        "        if self._node_hidden_sizes is None:\n",
        "            node_outputs = node_features\n",
        "        else:\n",
        "            node_outputs = self.MLP1(node_features)\n",
        "        if edge_features is None or self._edge_hidden_sizes is None:\n",
        "            edge_outputs = edge_features\n",
        "        else:\n",
        "            edge_outputs = self.MLP2(edge_features)\n",
        "\n",
        "        return node_outputs, edge_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRRsa-HwS7JC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nME4W8cXGAiD"
      },
      "source": [
        "#### The message passing layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brATNiuoVQQx"
      },
      "outputs": [],
      "source": [
        "def unsorted_segment_sum(data, segment_ids, num_segments):\n",
        "    \"\"\"\n",
        "    Computes the sum along segments of a tensor. Analogous to tf.unsorted_segment_sum.\n",
        "    :param data: A tensor whose segments are to be summed.\n",
        "    :param segment_ids: The segment indices tensor.\n",
        "    :param num_segments: The number of segments.\n",
        "    :return: A tensor of same data type as the data argument.\n",
        "    \"\"\"\n",
        "    # --------------print(\"\\ndata: {}\\nsegment_ids: {}\\nnum_segments: {}\".format(data, segment_ids, num_segments))\n",
        "    assert all([i in data.shape for i in segment_ids.shape]), \"segment_ids.shape should be a prefix of data.shape\"\n",
        "\n",
        "    # Encourage to use the below code when a deterministic result is\n",
        "    # needed (reproducibility). However, the code below is with low efficiency.\n",
        "\n",
        "    # tensor = torch.zeros(num_segments, data.shape[1]).cuda()\n",
        "    # for index in range(num_segments):\n",
        "    #     tensor[index, :] = torch.sum(data[segment_ids == index, :], dim=0)\n",
        "    # return tensor\n",
        "\n",
        "    #----------------print(\"\\ndata.shape:{} and segment_ids.shape: {}\".format(data.shape, segment_ids.shape))\n",
        "    if len(segment_ids.shape) == 1:\n",
        "      if torch.cuda.is_available():\n",
        "        s = torch.prod(torch.tensor(data.shape[1:])).long().cuda()\n",
        "        segment_ids = segment_ids.repeat_interleave(s).view(segment_ids.shape[0], *data.shape[1:])\n",
        "      else: \n",
        "        s = torch.prod(torch.tensor(data.shape[1:])).long()\n",
        "        segment_ids = segment_ids.repeat_interleave(s).view(segment_ids.shape[0], *data.shape[1:])\n",
        "    #----------------- print(\"\\ndata.shape:{} and segment_ids.shape: {}\".format(data.shape, segment_ids.shape))\n",
        "    assert data.shape == segment_ids.shape, \"data.shape and segment_ids.shape should be equal\"\n",
        "\n",
        "    shape = [num_segments] + list(data.shape[1:])\n",
        "    if torch.cuda.is_available():\n",
        "      tensor = torch.zeros(*shape).cuda().scatter_add(0, segment_ids, data) \n",
        "    else:\n",
        "      tensor = torch.zeros(*shape).scatter_add(0, segment_ids, data)\n",
        "\n",
        "    tensor = tensor.type(data.dtype)\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHAv-9aaG_4e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def graph_prop_once(node_states,\n",
        "                    from_idx,\n",
        "                    to_idx,\n",
        "                    message_net,\n",
        "                    aggregation_module=None,\n",
        "                    edge_features=None):\n",
        "    \"\"\"One round of propagation (message passing) in a graph.\n",
        "    Args:\n",
        "      node_states: [n_nodes, node_state_dim] float tensor, node state vectors, one\n",
        "        row for each node.\n",
        "      from_idx: [n_edges] int tensor, index of the from nodes.\n",
        "      to_idx: [n_edges] int tensor, index of the to nodes.\n",
        "      message_net: a network that maps concatenated edge inputs to message\n",
        "        vectors.\n",
        "      aggregation_module: a module that aggregates messages on edges to aggregated\n",
        "        messages for each node.  Should be a callable and can be called like the\n",
        "        following,\n",
        "        `aggregated_messages = aggregation_module(messages, to_idx, n_nodes)`,\n",
        "        where messages is [n_edges, edge_message_dim] tensor, to_idx is the index\n",
        "        of the to nodes, i.e. where each message should go to, and n_nodes is an\n",
        "        int which is the number of nodes to aggregate into.\n",
        "      edge_features: if provided, should be a [n_edges, edge_feature_dim] float\n",
        "        tensor, extra features for each edge.\n",
        "    Returns:\n",
        "      aggregated_messages: an [n_nodes, edge_message_dim] float tensor, the\n",
        "        aggregated messages, one row for each node.\n",
        "    \"\"\"\n",
        "    from_states = node_states[from_idx]\n",
        "    to_states = node_states[to_idx]\n",
        "    edge_inputs = [from_states, to_states]\n",
        "\n",
        "    if edge_features is not None:\n",
        "        edge_inputs.append(edge_features)\n",
        "\n",
        "    edge_inputs = torch.cat(edge_inputs, dim=-1)\n",
        "    messages = message_net(edge_inputs)\n",
        "    #-----------------------print(\"messages: {}\\nto_idx: {}\\n\".format(messages, to_idx))\n",
        "    tensor = unsorted_segment_sum(messages, to_idx, node_states.shape[0])\n",
        "    return tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXNIx_-rTnDd"
      },
      "outputs": [],
      "source": [
        "class GraphPropLayer(nn.Module):\n",
        "    \"\"\"Implementation of a graph propagation (message passing) layer.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 node_state_dim,\n",
        "                 edge_state_dim,\n",
        "                 edge_hidden_sizes,  # int\n",
        "                 node_hidden_sizes,  # int\n",
        "                 edge_net_init_scale=0.1,\n",
        "                 node_update_type='residual',\n",
        "                 use_reverse_direction=True,\n",
        "                 reverse_dir_param_different=True,\n",
        "                 layer_norm=False,\n",
        "                 prop_type='embedding',\n",
        "                 name='graph-net'):\n",
        "        \"\"\"Constructor.\n",
        "        Args:\n",
        "          node_state_dim: int, dimensionality of node states.\n",
        "          edge_hidden_sizes: list of ints, hidden sizes for the edge message\n",
        "            net, the last element in the list is the size of the message vectors.\n",
        "          node_hidden_sizes: list of ints, hidden sizes for the node update\n",
        "            net.\n",
        "          edge_net_init_scale: initialization scale for the edge networks.  This\n",
        "            is typically set to a small value such that the gradient does not blow\n",
        "            up.\n",
        "          node_update_type: type of node updates, one of {mlp, gru, residual}.\n",
        "          use_reverse_direction: set to True to also propagate messages in the\n",
        "            reverse direction.\n",
        "          reverse_dir_param_different: set to True to have the messages computed\n",
        "            using a different set of parameters than for the forward direction.\n",
        "          layer_norm: set to True to use layer normalization in a few places.\n",
        "          name: name of this module.\n",
        "        \"\"\"\n",
        "        super(GraphPropLayer, self).__init__()\n",
        "\n",
        "        self._node_state_dim = node_state_dim\n",
        "        self._edge_state_dim = edge_state_dim\n",
        "        self._edge_hidden_sizes = edge_hidden_sizes[:]\n",
        "\n",
        "        # output size is node_state_dim\n",
        "        self._node_hidden_sizes = node_hidden_sizes[:] + [node_state_dim]\n",
        "        self._edge_net_init_scale = edge_net_init_scale\n",
        "        self._node_update_type = node_update_type\n",
        "\n",
        "        self._use_reverse_direction = use_reverse_direction\n",
        "        self._reverse_dir_param_different = reverse_dir_param_different\n",
        "\n",
        "        self._layer_norm = layer_norm\n",
        "        self._prop_type = prop_type\n",
        "        self.build_model()\n",
        "\n",
        "        if self._layer_norm:\n",
        "            self.layer_norm1 = nn.LayerNorm()\n",
        "            self.layer_norm2 = nn.LayerNorm()\n",
        "\n",
        "    def build_model(self):\n",
        "        layer = []\n",
        "        layer.append(nn.Linear(self._node_state_dim*2 + self._edge_state_dim, self._edge_hidden_sizes[0]))\n",
        "        for i in range(1, len(self._edge_hidden_sizes)):\n",
        "            layer.append(nn.ReLU())\n",
        "            layer.append(nn.Linear(self._edge_hidden_sizes[i - 1], self._edge_hidden_sizes[i]))\n",
        "        self._message_net = nn.Sequential(*layer)\n",
        "\n",
        "        # optionally compute message vectors in the reverse direction\n",
        "        if self._use_reverse_direction:\n",
        "            if self._reverse_dir_param_different:\n",
        "                layer = []\n",
        "                layer.append(nn.Linear(self._node_state_dim*2 + self._edge_state_dim, self._edge_hidden_sizes[0]))\n",
        "                for i in range(1, len(self._edge_hidden_sizes)):\n",
        "                    layer.append(nn.ReLU())\n",
        "                    layer.append(nn.Linear(self._edge_hidden_sizes[i - 1], self._edge_hidden_sizes[i]))\n",
        "                self._reverse_message_net = nn.Sequential(*layer)\n",
        "            else:\n",
        "                self._reverse_message_net = self._message_net\n",
        "\n",
        "        if self._node_update_type == 'gru':\n",
        "            if self._prop_type == 'embedding':\n",
        "                self.GRU = torch.nn.GRU(self._node_state_dim * 2, self._node_state_dim)\n",
        "            elif self._prop_type == 'matching':\n",
        "                self.GRU = torch.nn.GRU(self._node_state_dim * 3, self._node_state_dim)\n",
        "        else:\n",
        "            layer = []\n",
        "            if self._prop_type == 'embedding':\n",
        "                layer.append(nn.Linear(self._node_state_dim * 3, self._node_hidden_sizes[0]))\n",
        "            elif self._prop_type == 'matching':\n",
        "                layer.append(nn.Linear(self._node_state_dim * 4, self._node_hidden_sizes[0]))\n",
        "            for i in range(1, len(self._node_hidden_sizes)):\n",
        "                layer.append(nn.ReLU())\n",
        "                layer.append(nn.Linear(self._node_hidden_sizes[i - 1], self._node_hidden_sizes[i]))\n",
        "            self.MLP = nn.Sequential(*layer)\n",
        "\n",
        "    def _compute_aggregated_messages(\n",
        "            self, node_states, from_idx, to_idx, edge_features=None):\n",
        "        \"\"\"Compute aggregated messages for each node.\n",
        "        Args:\n",
        "          node_states: [n_nodes, input_node_state_dim] float tensor, node states.\n",
        "          from_idx: [n_edges] int tensor, from node indices for each edge.\n",
        "          to_idx: [n_edges] int tensor, to node indices for each edge.\n",
        "          edge_features: if not None, should be [n_edges, edge_embedding_dim]\n",
        "            tensor, edge features.\n",
        "        Returns:\n",
        "          aggregated_messages: [n_nodes, aggregated_message_dim] float tensor, the\n",
        "            aggregated messages for each node.\n",
        "        \"\"\"\n",
        "\n",
        "        aggregated_messages = graph_prop_once(\n",
        "            node_states,\n",
        "            from_idx,\n",
        "            to_idx,\n",
        "            self._message_net,\n",
        "            aggregation_module=None,\n",
        "            edge_features=edge_features)\n",
        "\n",
        "        # optionally compute message vectors in the reverse direction\n",
        "        if self._use_reverse_direction:\n",
        "            reverse_aggregated_messages = graph_prop_once(\n",
        "                node_states,\n",
        "                to_idx,\n",
        "                from_idx,\n",
        "                self._reverse_message_net,\n",
        "                aggregation_module=None,\n",
        "                edge_features=edge_features)\n",
        "\n",
        "            aggregated_messages += reverse_aggregated_messages\n",
        "\n",
        "        if self._layer_norm:\n",
        "            aggregated_messages = self.layer_norm1(aggregated_messages)\n",
        "\n",
        "        return aggregated_messages\n",
        "\n",
        "    def _compute_node_update(self,\n",
        "                             node_states,\n",
        "                             node_state_inputs,\n",
        "                             node_features=None):\n",
        "        \"\"\"Compute node updates.\n",
        "        Args:\n",
        "          node_states: [n_nodes, node_state_dim] float tensor, the input node\n",
        "            states.\n",
        "          node_state_inputs: a list of tensors used to compute node updates.  Each\n",
        "            element tensor should have shape [n_nodes, feat_dim], where feat_dim can\n",
        "            be different.  These tensors will be concatenated along the feature\n",
        "            dimension.\n",
        "          node_features: extra node features if provided, should be of size\n",
        "            [n_nodes, extra_node_feat_dim] float tensor, can be used to implement\n",
        "            different types of skip connections.\n",
        "        Returns:\n",
        "          new_node_states: [n_nodes, node_state_dim] float tensor, the new node\n",
        "            state tensor.\n",
        "        Raises:\n",
        "          ValueError: if node update type is not supported.\n",
        "        \"\"\"\n",
        "        if self._node_update_type in ('mlp', 'residual'):\n",
        "            node_state_inputs.append(node_states)\n",
        "        if node_features is not None:\n",
        "            node_state_inputs.append(node_features)\n",
        "\n",
        "        if len(node_state_inputs) == 1:\n",
        "            node_state_inputs = node_state_inputs[0]\n",
        "        else:\n",
        "            node_state_inputs = torch.cat(node_state_inputs, dim=-1)\n",
        "\n",
        "        if self._node_update_type == 'gru':\n",
        "            node_state_inputs = torch.unsqueeze(node_state_inputs, 0)\n",
        "            node_states = torch.unsqueeze(node_states, 0)\n",
        "            _, new_node_states = self.GRU(node_state_inputs, node_states)\n",
        "            new_node_states = torch.squeeze(new_node_states)\n",
        "            return new_node_states\n",
        "        else:\n",
        "            mlp_output = self.MLP(node_state_inputs)\n",
        "            if self._layer_norm:\n",
        "                mlp_output = nn.self.layer_norm2(mlp_output)\n",
        "            if self._node_update_type == 'mlp':\n",
        "                return mlp_output\n",
        "            elif self._node_update_type == 'residual':\n",
        "                return node_states + mlp_output\n",
        "            else:\n",
        "                raise ValueError('Unknown node update type %s' % self._node_update_type)\n",
        "\n",
        "    def forward(self,\n",
        "                node_states,\n",
        "                from_idx,\n",
        "                to_idx,\n",
        "                edge_features=None,\n",
        "                node_features=None):\n",
        "        \"\"\"Run one propagation step.\n",
        "        Args:\n",
        "          node_states: [n_nodes, input_node_state_dim] float tensor, node states.\n",
        "          from_idx: [n_edges] int tensor, from node indices for each edge.\n",
        "          to_idx: [n_edges] int tensor, to node indices for each edge.\n",
        "          edge_features: if not None, should be [n_edges, edge_embedding_dim]\n",
        "            tensor, edge features.\n",
        "          node_features: extra node features if provided, should be of size\n",
        "            [n_nodes, extra_node_feat_dim] float tensor, can be used to implement\n",
        "            different types of skip connections.\n",
        "        Returns:\n",
        "          node_states: [n_nodes, node_state_dim] float tensor, new node states.\n",
        "        \"\"\"\n",
        "        aggregated_messages = self._compute_aggregated_messages(\n",
        "            node_states, from_idx, to_idx, edge_features=edge_features)\n",
        "\n",
        "        return self._compute_node_update(node_states,\n",
        "                                         [aggregated_messages],\n",
        "                                         node_features=node_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jtiaHTOMZCi"
      },
      "source": [
        "#### Graph aggregator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fWleJosMeip"
      },
      "outputs": [],
      "source": [
        "class GraphAggregator(nn.Module):\n",
        "    \"\"\"This module computes graph representations by aggregating from parts.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 node_hidden_sizes,\n",
        "                 graph_transform_sizes=None,\n",
        "                 input_size=None,\n",
        "                 gated=True,\n",
        "                 aggregation_type='sum',\n",
        "                 name='graph-aggregator'):\n",
        "        \"\"\"Constructor.\n",
        "        Args:\n",
        "          node_hidden_sizes: the hidden layer sizes of the node transformation nets.\n",
        "            The last element is the size of the aggregated graph representation.\n",
        "          graph_transform_sizes: sizes of the transformation layers on top of the\n",
        "            graph representations.  The last element of this list is the final\n",
        "            dimensionality of the output graph representations.\n",
        "          gated: set to True to do gated aggregation, False not to.\n",
        "          aggregation_type: one of {sum, max, mean, sqrt_n}.\n",
        "          name: name of this module.\n",
        "        \"\"\"\n",
        "        super(GraphAggregator, self).__init__()\n",
        "\n",
        "        self._node_hidden_sizes = node_hidden_sizes\n",
        "        self._graph_transform_sizes = graph_transform_sizes\n",
        "        self._graph_state_dim = node_hidden_sizes[-1]\n",
        "        self._input_size = input_size\n",
        "        #  The last element is the size of the aggregated graph representation.\n",
        "        self._gated = gated\n",
        "        self._aggregation_type = aggregation_type\n",
        "        self._aggregation_op = None\n",
        "        self.MLP1, self.MLP2 = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        node_hidden_sizes = self._node_hidden_sizes\n",
        "        if self._gated:\n",
        "            node_hidden_sizes[-1] = self._graph_state_dim * 2\n",
        "\n",
        "        layer = []\n",
        "        layer.append(nn.Linear(self._input_size[0], node_hidden_sizes[0]))\n",
        "        for i in range(1, len(node_hidden_sizes)):\n",
        "            layer.append(nn.ReLU())\n",
        "            layer.append(nn.Linear(node_hidden_sizes[i - 1], node_hidden_sizes[i]))\n",
        "        MLP1 = nn.Sequential(*layer)\n",
        "\n",
        "        if (self._graph_transform_sizes is not None and\n",
        "                len(self._graph_transform_sizes) > 0):\n",
        "            layer = []\n",
        "            layer.append(nn.Linear(self._graph_state_dim, self._graph_transform_sizes[0]))\n",
        "            for i in range(1, len(self._graph_transform_sizes)):\n",
        "                layer.append(nn.ReLU())\n",
        "                layer.append(nn.Linear(self._graph_transform_sizes[i - 1], self._graph_transform_sizes[i]))\n",
        "            MLP2 = nn.Sequential(*layer)\n",
        "\n",
        "        return MLP1, MLP2\n",
        "\n",
        "    def forward(self, node_states, graph_idx, n_graphs):\n",
        "        \"\"\"Compute aggregated graph representations.\n",
        "        Args:\n",
        "          node_states: [n_nodes, node_state_dim] float tensor, node states of a\n",
        "            batch of graphs concatenated together along the first dimension.\n",
        "          graph_idx: [n_nodes] int tensor, graph ID for each node.\n",
        "          n_graphs: integer, number of graphs in this batch.\n",
        "        Returns:\n",
        "          graph_states: [n_graphs, graph_state_dim] float tensor, graph\n",
        "            representations, one row for each graph.\n",
        "        \"\"\"\n",
        "\n",
        "        node_states_g = self.MLP1(node_states)\n",
        "\n",
        "        if self._gated:\n",
        "            gates = torch.sigmoid(node_states_g[:, :self._graph_state_dim])\n",
        "            node_states_g = node_states_g[:, self._graph_state_dim:] * gates\n",
        "\n",
        "        graph_states = unsorted_segment_sum(node_states_g, graph_idx, n_graphs)\n",
        "\n",
        "        if self._aggregation_type == 'max':\n",
        "            # reset everything that's smaller than -1e5 to 0.\n",
        "            graph_states *= torch.FloatTensor(graph_states > -1e5)\n",
        "        # transform the reduced graph states further\n",
        "\n",
        "\n",
        "        if (self._graph_transform_sizes is not None and\n",
        "                len(self._graph_transform_sizes) > 0):\n",
        "            graph_states = self.MLP2(graph_states)\n",
        "\n",
        "        return graph_states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi_kV7PwOq_n"
      },
      "source": [
        "#### Putting them together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0H8kbRXOtce"
      },
      "outputs": [],
      "source": [
        "class GraphEmbeddingNet(nn.Module):\n",
        "    \"\"\"A graph to embedding mapping network.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 encoder,\n",
        "                 aggregator,\n",
        "                 node_state_dim,\n",
        "                 edge_state_dim,\n",
        "                 edge_hidden_sizes,\n",
        "                 node_hidden_sizes,\n",
        "                 n_prop_layers,\n",
        "                 share_prop_params=False,\n",
        "                 edge_net_init_scale=0.1,\n",
        "                 node_update_type='residual',\n",
        "                 use_reverse_direction=True,\n",
        "                 reverse_dir_param_different=True,\n",
        "                 layer_norm=False,\n",
        "                 layer_class=GraphPropLayer,\n",
        "                 prop_type='embedding',\n",
        "                 name='graph-embedding-net'):\n",
        "        \"\"\"Constructor.\n",
        "        Args:\n",
        "          encoder: GraphEncoder, encoder that maps features to embeddings.\n",
        "          aggregator: GraphAggregator, aggregator that produces graph\n",
        "            representations.\n",
        "          node_state_dim: dimensionality of node states.\n",
        "          edge_hidden_sizes: sizes of the hidden layers of the edge message nets.\n",
        "          node_hidden_sizes: sizes of the hidden layers of the node update nets.\n",
        "          n_prop_layers: number of graph propagation layers.\n",
        "          share_prop_params: set to True to share propagation parameters across all\n",
        "            graph propagation layers, False not to.\n",
        "          edge_net_init_scale: scale of initialization for the edge message nets.\n",
        "          node_update_type: type of node updates, one of {mlp, gru, residual}.\n",
        "          use_reverse_direction: set to True to also propagate messages in the\n",
        "            reverse direction.\n",
        "          reverse_dir_param_different: set to True to have the messages computed\n",
        "            using a different set of parameters than for the forward direction.\n",
        "          layer_norm: set to True to use layer normalization in a few places.\n",
        "          name: name of this module.\n",
        "        \"\"\"\n",
        "        super(GraphEmbeddingNet, self).__init__()\n",
        "\n",
        "        self._encoder = encoder\n",
        "        self._aggregator = aggregator\n",
        "        self._node_state_dim = node_state_dim\n",
        "        self._edge_state_dim = edge_state_dim\n",
        "        self._edge_hidden_sizes = edge_hidden_sizes\n",
        "        self._node_hidden_sizes = node_hidden_sizes\n",
        "        self._n_prop_layers = n_prop_layers\n",
        "        self._share_prop_params = share_prop_params\n",
        "        self._edge_net_init_scale = edge_net_init_scale\n",
        "        self._node_update_type = node_update_type\n",
        "        self._use_reverse_direction = use_reverse_direction\n",
        "        self._reverse_dir_param_different = reverse_dir_param_different\n",
        "        self._layer_norm = layer_norm\n",
        "        self._prop_layers = []\n",
        "        self._prop_layers = nn.ModuleList()\n",
        "        self._layer_class = layer_class\n",
        "        self._prop_type = prop_type\n",
        "        self.build_model()\n",
        "\n",
        "    def _build_layer(self, layer_id):\n",
        "        \"\"\"Build one layer in the network.\"\"\"\n",
        "        return self._layer_class(\n",
        "            self._node_state_dim,\n",
        "            self._edge_state_dim,\n",
        "            self._edge_hidden_sizes,\n",
        "            self._node_hidden_sizes,\n",
        "            edge_net_init_scale=self._edge_net_init_scale,\n",
        "            node_update_type=self._node_update_type,\n",
        "            use_reverse_direction=self._use_reverse_direction,\n",
        "            reverse_dir_param_different=self._reverse_dir_param_different,\n",
        "            layer_norm=self._layer_norm,\n",
        "            prop_type=self._prop_type)\n",
        "        # name='graph-prop-%d' % layer_id)\n",
        "\n",
        "    def _apply_layer(self,\n",
        "                     layer,\n",
        "                     node_states,\n",
        "                     from_idx,\n",
        "                     to_idx,\n",
        "                     graph_idx,\n",
        "                     n_graphs,\n",
        "                     edge_features):\n",
        "        \"\"\"Apply one layer on the given inputs.\"\"\"\n",
        "        del graph_idx, n_graphs\n",
        "        return layer(node_states, from_idx, to_idx, edge_features=edge_features)\n",
        "\n",
        "    def build_model(self):\n",
        "        if len(self._prop_layers) < self._n_prop_layers:\n",
        "            # build the layers\n",
        "            for i in range(self._n_prop_layers):\n",
        "                if i == 0 or not self._share_prop_params:\n",
        "                    layer = self._build_layer(i)\n",
        "                else:\n",
        "                    layer = self._prop_layers[0]\n",
        "                self._prop_layers.append(layer)\n",
        "\n",
        "    def forward(self,\n",
        "                node_features,\n",
        "                edge_features,\n",
        "                from_idx,\n",
        "                to_idx,\n",
        "                graph_idx,\n",
        "                n_graphs):\n",
        "        \"\"\"Compute graph representations.\n",
        "        Args:\n",
        "          node_features: [n_nodes, node_feat_dim] float tensor.\n",
        "          edge_features: [n_edges, edge_feat_dim] float tensor.\n",
        "          from_idx: [n_edges] int tensor, index of the from node for each edge.\n",
        "          to_idx: [n_edges] int tensor, index of the to node for each edge.\n",
        "          graph_idx: [n_nodes] int tensor, graph id for each node.\n",
        "          n_graphs: int, number of graphs in the batch.\n",
        "        Returns:\n",
        "          graph_representations: [n_graphs, graph_representation_dim] float tensor,\n",
        "            graph representations.\n",
        "        \"\"\"\n",
        "\n",
        "        node_features, edge_features = self._encoder(node_features, edge_features)\n",
        "        node_states = node_features\n",
        "\n",
        "        layer_outputs = [node_states]\n",
        "\n",
        "        for layer in self._prop_layers:\n",
        "            # node_features could be wired in here as well, leaving it out for now as\n",
        "            # it is already in the inputs\n",
        "            node_states = self._apply_layer(\n",
        "                layer,\n",
        "                node_states,\n",
        "                from_idx,\n",
        "                to_idx,\n",
        "                graph_idx,\n",
        "                n_graphs,\n",
        "                edge_features)\n",
        "            layer_outputs.append(node_states)\n",
        "\n",
        "        # these tensors may be used e.g. for visualization\n",
        "        self._layer_outputs = layer_outputs\n",
        "        return self._aggregator(node_states, graph_idx, n_graphs)\n",
        "\n",
        "    def reset_n_prop_layers(self, n_prop_layers):\n",
        "        \"\"\"Set n_prop_layers to the provided new value.\n",
        "        This allows us to train with certain number of propagation layers and\n",
        "        evaluate with a different number of propagation layers.\n",
        "        This only works if n_prop_layers is smaller than the number used for\n",
        "        training, or when share_prop_params is set to True, in which case this can\n",
        "        be arbitrarily large.\n",
        "        Args:\n",
        "          n_prop_layers: the new number of propagation layers to set.\n",
        "        \"\"\"\n",
        "        self._n_prop_layers = n_prop_layers\n",
        "\n",
        "    @property\n",
        "    def n_prop_layers(self):\n",
        "        return self._n_prop_layers\n",
        "\n",
        "    def get_layer_outputs(self):\n",
        "        \"\"\"Get the outputs at each layer.\"\"\"\n",
        "        if hasattr(self, '_layer_outputs'):\n",
        "            return self._layer_outputs\n",
        "        else:\n",
        "            raise ValueError('No layer outputs available.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXMSHIbdeAxt"
      },
      "source": [
        "### Graph matching networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N2RBfw3lAeM"
      },
      "source": [
        "#### A few similarity functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N2rDBx_-Xgd"
      },
      "source": [
        "$$d(G_1, G_2) = d_H(embed\\_and\\_match(G_1, G_2))$$\n",
        "\n",
        "1. $embed\\_and\\_match(G_1, G_2)$ =\n",
        "\n",
        "1) mapping: \n",
        "\n",
        "$$\\begin{array}{rcl}\n",
        "h_i^{(0)} &=& \\mathrm{MLP_{node}}(x_i) \\\\\n",
        "e_{ij} &=& \\mathrm{MLP_{edge}}(x_{ij})\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "2) passing cross-graph messages: \n",
        "$$\\begin{array}{rcl}\n",
        "m_{i\\rightarrow j} &=& f_\\mathrm{message}(h_i^{(t)}, h_j^{(t)}, e_{ij}) \\\\\n",
        "\\end{array}\n",
        "$$\n",
        "cross graph attention weight:\n",
        "$$\\begin{array}{rcl}\n",
        "a_{i\\rightarrow j} &=& \\frac{\\exp(s(h_i^{(t)}, h_j^{(t)}))}{\\sum_j \\exp(s(h_i^{(t)}, h_j^{(t)}))} , GAT: a_{i\\rightarrow j} &=& \\frac{\\exp(LeakyReLU([Wh_i||Wh_j]))}{\\sum_j \\exp(LeakyReLU([Wh_i||Wh_j]))}\\\\\n",
        "a_{j\\rightarrow i} &=& \\frac{\\exp(s(h_i^{(t)}, h_j^{(t)}))}{\\sum_i \\exp(s(h_i^{(t)}, h_j^{(t)}))}\n",
        "\\end{array}\n",
        "$$\n",
        "take the difference of attention-weighted sum of node representations:\n",
        "$$\\begin{array}{rcl}\n",
        "\\mu_i &=& \\sum_j a_{i\\rightarrow j} (h_i^{(t)} - h_j^{(t)}) = h_i^{(t)} - \\sum_j a_{i\\rightarrow j} h_j^{(t)}, GAT: \\mu_i &=& \\sum_j a_{i\\rightarrow j} Wh_i^{(t)}\\\\\n",
        "\\mu_j &=& \\sum_i a_{j\\rightarrow i} (h_j^{(t)} - h_i^{(t)}) = h_j^{(t)} - \\sum_i a_{j\\rightarrow i} h_i^{(t)}.\n",
        "\\end{array}\n",
        "$$\n",
        "$$\n",
        "h_i^{(t+1)} = f_\\mathrm{node}\\left(h_i^{(t)}, \\sum_{j:(j,i)\\in E} m_{j\\rightarrow i}, \\mu_i\\right). GAT: h_i^{(t+1)} = \\sigma \\left(\\mu_i\\right)\n",
        "$$\n",
        "3) aggregate node representations to get graph representations\n",
        "\n",
        "$$h_G = \\mathrm{MLP_G}\\left(\\sum_{i\\in V} h_i^{(T)}\\right).$$\n",
        "$$h_G = \\mathrm{MLP_G}\\left(\\sum_{i\\in V} \\sigma(\\mathrm{MLP_{gate}}(h_i^{(T)})) \\odot \\mathrm{MLP}(h_i^{(T)})\\right).$$\n",
        "\n",
        "2. $d_H$ is an existing distance metric\\:\n",
        "\n",
        "Euclidean distance: \n",
        "\n",
        "$d_H(x, y) = \\sqrt{\\sum_{i=1}^H (x_i - y_i)^2}$\n",
        "\n",
        "or Hamming distance: \n",
        "\n",
        "$d_H(x, y)=\\sum_{i=1}^H \\mathbb{I}[x_i \\ne y_i]$\n",
        "\n",
        "or dot product:\n",
        "\n",
        "or cosine:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40I5l40klFFH"
      },
      "outputs": [],
      "source": [
        "def pairwise_euclidean_similarity(x, y):\n",
        "    \"\"\"Compute the pairwise Euclidean similarity between x and y.\n",
        "    This function computes the following similarity value between each pair of x_i\n",
        "    and y_j: s(x_i, y_j) = -|x_i - y_j|^2.\n",
        "    Args:\n",
        "      x: NxD float tensor.\n",
        "      y: MxD float tensor.\n",
        "    Returns:\n",
        "      s: NxM float tensor, the pairwise euclidean similarity.\n",
        "    \"\"\"\n",
        "    s = 2 * torch.mm(x, torch.transpose(y, 1, 0))\n",
        "    diag_x = torch.sum(x * x, dim=-1)\n",
        "    diag_x = torch.unsqueeze(diag_x, 0)\n",
        "    diag_y = torch.reshape(torch.sum(y * y, dim=-1), (1, -1))\n",
        "\n",
        "    return s - diag_x - diag_y\n",
        "\n",
        "\n",
        "def pairwise_dot_product_similarity(x, y):\n",
        "    \"\"\"Compute the dot product similarity between x and y.\n",
        "    This function computes the following similarity value between each pair of x_i\n",
        "    and y_j: s(x_i, y_j) = x_i^T y_j.\n",
        "    Args:\n",
        "      x: NxD float tensor.\n",
        "      y: MxD float tensor.\n",
        "    Returns:\n",
        "      s: NxM float tensor, the pairwise dot product similarity.\n",
        "    \"\"\"\n",
        "    return torch.mm(x, torch.transpose(y, 1, 0))\n",
        "\n",
        "\n",
        "def pairwise_cosine_similarity(x, y):\n",
        "    \"\"\"Compute the cosine similarity between x and y.\n",
        "    This function computes the following similarity value between each pair of x_i\n",
        "    and y_j: s(x_i, y_j) = x_i^T y_j / (|x_i||y_j|).\n",
        "    Args:\n",
        "      x: NxD float tensor.\n",
        "      y: MxD float tensor.\n",
        "    Returns:\n",
        "      s: NxM float tensor, the pairwise cosine similarity.\n",
        "    \"\"\"\n",
        "    x = torch.div(x, torch.sqrt(torch.max(torch.sum(x ** 2), 1e-12)))\n",
        "    y = torch.div(y, torch.sqrt(torch.max(torch.sum(y ** 2), 1e-12)))\n",
        "    return torch.mm(x, torch.transpose(y, 1, 0))\n",
        "\n",
        "\n",
        "PAIRWISE_SIMILARITY_FUNCTION = {\n",
        "    'euclidean': pairwise_euclidean_similarity,\n",
        "    'dotproduct': pairwise_dot_product_similarity,\n",
        "    'cosine': pairwise_cosine_similarity,\n",
        "}\n",
        "\n",
        "\n",
        "def get_pairwise_similarity(name):\n",
        "    \"\"\"Get pairwise similarity metric by name.\n",
        "    Args:\n",
        "      name: string, name of the similarity metric, one of {dot-product, cosine,\n",
        "        euclidean}.\n",
        "    Returns:\n",
        "      similarity: a (x, y) -> sim function.\n",
        "    Raises:\n",
        "      ValueError: if name is not supported.\n",
        "    \"\"\"\n",
        "    if name not in PAIRWISE_SIMILARITY_FUNCTION:\n",
        "        raise ValueError('Similarity metric name \"%s\" not supported.' % name)\n",
        "    else:\n",
        "        return PAIRWISE_SIMILARITY_FUNCTION[name]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgMsWfgaldLI"
      },
      "source": [
        "#### Cross-graph attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1gJmQLmlcv6"
      },
      "outputs": [],
      "source": [
        "def compute_cross_attention(x, y, sim):\n",
        "    \"\"\"Compute cross attention.\n",
        "    x_i attend to y_j:\n",
        "    a_{i->j} = exp(sim(x_i, y_j)) / sum_j exp(sim(x_i, y_j))\n",
        "    y_j attend to x_i:\n",
        "    a_{j->i} = exp(sim(x_i, y_j)) / sum_i exp(sim(x_i, y_j))\n",
        "    attention_x = sum_j a_{i->j} y_j\n",
        "    attention_y = sum_i a_{j->i} x_i\n",
        "    Args:\n",
        "      x: NxD float tensor.\n",
        "      y: MxD float tensor.\n",
        "      sim: a (x, y) -> similarity function.\n",
        "    Returns:\n",
        "      attention_x: NxD float tensor.\n",
        "      attention_y: NxD float tensor.\n",
        "    \"\"\"\n",
        "    a = sim(x, y)\n",
        "    a_x = torch.softmax(a, dim=1)  # i->j\n",
        "    a_y = torch.softmax(a, dim=0)  # j->i\n",
        "    attention_x = torch.mm(a_x, y)\n",
        "    attention_y = torch.mm(torch.transpose(a_y, 1, 0), x)\n",
        "    return attention_x, attention_y\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT59ls93Xabf"
      },
      "outputs": [],
      "source": [
        "def batch_block_pair_attention(data,\n",
        "                               block_idx,\n",
        "                               n_blocks,\n",
        "                               similarity='dotproduct'):\n",
        "    \"\"\"Compute batched attention between pairs of blocks.\n",
        "    This function partitions the batch data into blocks according to block_idx.\n",
        "    For each pair of blocks, x = data[block_idx == 2i], and\n",
        "    y = data[block_idx == 2i+1], we compute\n",
        "    x_i attend to y_j:\n",
        "    a_{i->j} = exp(sim(x_i, y_j)) / sum_j exp(sim(x_i, y_j))\n",
        "    y_j attend to x_i:\n",
        "    a_{j->i} = exp(sim(x_i, y_j)) / sum_i exp(sim(x_i, y_j))\n",
        "    and\n",
        "    attention_x = sum_j a_{i->j} y_j\n",
        "    attention_y = sum_i a_{j->i} x_i.\n",
        "    Args:\n",
        "      data: NxD float tensor.\n",
        "      block_idx: N-dim int tensor.\n",
        "      n_blocks: integer.\n",
        "      similarity: a string, the similarity metric.\n",
        "    Returns:\n",
        "      attention_output: NxD float tensor, each x_i replaced by attention_x_i.\n",
        "    Raises:\n",
        "      ValueError: if n_blocks is not an integer or not a multiple of 2.\n",
        "    \"\"\"\n",
        "    if not isinstance(n_blocks, int):\n",
        "        raise ValueError('n_blocks (%s) has to be an integer.' % str(n_blocks))\n",
        "\n",
        "    if n_blocks % 2 != 0:\n",
        "        raise ValueError('n_blocks (%d) must be a multiple of 2.' % n_blocks)\n",
        "\n",
        "    sim = get_pairwise_similarity(similarity)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # This is probably better than doing boolean_mask for each i\n",
        "    partitions = []\n",
        "    for i in range(n_blocks):\n",
        "        partitions.append(data[block_idx == i, :])\n",
        "\n",
        "    for i in range(0, n_blocks, 2):\n",
        "        x = partitions[i]\n",
        "        y = partitions[i + 1]\n",
        "        attention_x, attention_y = compute_cross_attention(x, y, sim)\n",
        "        results.append(attention_x)\n",
        "        results.append(attention_y)\n",
        "    results = torch.cat(results, dim=0)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epm3EuA0ltiQ"
      },
      "source": [
        "#### Graph matching layer and graph matching networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k85vRuhlueA"
      },
      "outputs": [],
      "source": [
        "class GraphPropMatchingLayer(GraphPropLayer):\n",
        "    \"\"\"A graph propagation layer that also does cross graph matching.\n",
        "    It assumes the incoming graph data is batched and paired, i.e. graph 0 and 1\n",
        "    forms the first pair and graph 2 and 3 are the second pair etc., and computes\n",
        "    cross-graph attention-based matching for each pair.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self,\n",
        "                node_states,\n",
        "                from_idx,\n",
        "                to_idx,\n",
        "                graph_idx,\n",
        "                n_graphs,\n",
        "                similarity='dotproduct',\n",
        "                edge_features=None,\n",
        "                node_features=None):\n",
        "        \"\"\"Run one propagation step with cross-graph matching.\n",
        "        Args:\n",
        "          node_states: [n_nodes, node_state_dim] float tensor, node states.\n",
        "          from_idx: [n_edges] int tensor, from node indices for each edge.\n",
        "          to_idx: [n_edges] int tensor, to node indices for each edge.\n",
        "          graph_idx: [n_onodes] int tensor, graph id for each node.\n",
        "          n_graphs: integer, number of graphs in the batch.\n",
        "          similarity: type of similarity to use for the cross graph attention.\n",
        "          edge_features: if not None, should be [n_edges, edge_feat_dim] tensor,\n",
        "            extra edge features.\n",
        "          node_features: if not None, should be [n_nodes, node_feat_dim] tensor,\n",
        "            extra node features.\n",
        "        Returns:\n",
        "          node_states: [n_nodes, node_state_dim] float tensor, new node states.\n",
        "        Raises:\n",
        "          ValueError: if some options are not provided correctly.\n",
        "        \"\"\"\n",
        "        aggregated_messages = self._compute_aggregated_messages(\n",
        "            node_states, from_idx, to_idx, edge_features=edge_features)\n",
        "\n",
        "        cross_graph_attention = batch_block_pair_attention(\n",
        "            node_states, graph_idx, n_graphs, similarity=similarity)\n",
        "        attention_input = node_states - cross_graph_attention\n",
        "\n",
        "        return self._compute_node_update(node_states,\n",
        "                                         [aggregated_messages, attention_input],\n",
        "                                         node_features=node_features)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2SlI1H-XVTS"
      },
      "outputs": [],
      "source": [
        "class GraphMatchingNet(GraphEmbeddingNet):\n",
        "    \"\"\"Graph matching net.\n",
        "    This class uses graph matching layers instead of the simple graph prop layers.\n",
        "    It assumes the incoming graph data is batched and paired, i.e. graph 0 and 1\n",
        "    forms the first pair and graph 2 and 3 are the second pair etc., and computes\n",
        "    cross-graph attention-based matching for each pair.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 encoder,\n",
        "                 aggregator,\n",
        "                 node_state_dim,\n",
        "                 edge_state_dim,\n",
        "                 edge_hidden_sizes,\n",
        "                 node_hidden_sizes,\n",
        "                 n_prop_layers,\n",
        "                 share_prop_params=False,\n",
        "                 edge_net_init_scale=0.1,\n",
        "                 node_update_type='residual',\n",
        "                 use_reverse_direction=True,\n",
        "                 reverse_dir_param_different=True,\n",
        "                 layer_norm=False,\n",
        "                 layer_class=GraphPropLayer,\n",
        "                 similarity='dotproduct',\n",
        "                 prop_type='embedding'):\n",
        "        super(GraphMatchingNet, self).__init__(\n",
        "            encoder,\n",
        "            aggregator,\n",
        "            node_state_dim,\n",
        "            edge_state_dim,\n",
        "            edge_hidden_sizes,\n",
        "            node_hidden_sizes,\n",
        "            n_prop_layers,\n",
        "            share_prop_params=share_prop_params,\n",
        "            edge_net_init_scale=edge_net_init_scale,\n",
        "            node_update_type=node_update_type,\n",
        "            use_reverse_direction=use_reverse_direction,\n",
        "            reverse_dir_param_different=reverse_dir_param_different,\n",
        "            layer_norm=layer_norm,\n",
        "            layer_class=GraphPropMatchingLayer,\n",
        "            prop_type=prop_type,\n",
        "        )\n",
        "        self._similarity = similarity\n",
        "\n",
        "    def _apply_layer(self,\n",
        "                     layer,\n",
        "                     node_states,\n",
        "                     from_idx,\n",
        "                     to_idx,\n",
        "                     graph_idx,\n",
        "                     n_graphs,\n",
        "                     edge_features):\n",
        "        \"\"\"Apply one layer on the given inputs.\"\"\"\n",
        "        return layer(node_states, from_idx, to_idx, graph_idx, n_graphs,\n",
        "                     similarity=self._similarity, edge_features=edge_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgNe7Snlm_bS"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t1JU0w6nG83"
      },
      "source": [
        "### Training on pairs (loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvplK0vcTy0g"
      },
      "source": [
        "Euclidean distance: $d_H(x, y) = \\sqrt{\\sum_{i=1}^H (x_i - y_i)^2}$,\n",
        "\n",
        "or Hamming distance: $d_H(x, y)=\\sum_{i=1}^H \\mathbb{I}[x_i \\ne y_i]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpA1uBcCnNUd"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(x, y):\n",
        "    \"\"\"This is the squared Euclidean distance.\"\"\"\n",
        "    return torch.sum((x - y) ** 2, dim=-1)\n",
        "\n",
        "\n",
        "def approximate_hamming_similarity(x, y):\n",
        "    \"\"\"Approximate Hamming similarity.\"\"\"\n",
        "    return torch.mean(torch.tanh(x) * torch.tanh(y), dim=1)\n",
        "\n",
        "\n",
        "def pairwise_loss(x, y, labels, loss_type='margin', margin=1.0):\n",
        "    \"\"\"Compute pairwise loss.\n",
        "    Args:\n",
        "      x: [N, D] float tensor, representations for N examples.\n",
        "      y: [N, D] float tensor, representations for another N examples.\n",
        "      labels: [N] int tensor, with values in -1 or +1.  labels[i] = +1 if x[i]\n",
        "        and y[i] are similar, and -1 otherwise.\n",
        "      loss_type: margin or hamming.\n",
        "      margin: float scalar, margin for the margin loss.\n",
        "    Returns:\n",
        "      loss: [N] float tensor.  Loss for each pair of representations.\n",
        "    \"\"\"\n",
        "\n",
        "    labels = labels.float()\n",
        "\n",
        "    if loss_type == 'margin':\n",
        "        return torch.relu(margin - labels * (1 - euclidean_distance(x, y)))\n",
        "    elif loss_type == 'hamming':\n",
        "        return 0.25 * (labels - approximate_hamming_similarity(x, y)) ** 2\n",
        "    else:\n",
        "        raise ValueError('Unknown loss_type %s' % loss_type)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hes6_XttnumL"
      },
      "source": [
        "### Training on triplets (loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA0gQRIXWhZB"
      },
      "source": [
        "margin, hamming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nQlkCNsn1oC"
      },
      "outputs": [],
      "source": [
        "def triplet_loss(x_1, y, x_2, z, loss_type='margin', margin=1.0):\n",
        "    \"\"\"Compute triplet loss.\n",
        "    This function computes loss on a triplet of inputs (x, y, z).  A similarity or\n",
        "    distance value is computed for each pair of (x, y) and (x, z).  Since the\n",
        "    representations for x can be different in the two pairs (like our matching\n",
        "    model) we distinguish the two x representations by x_1 and x_2.\n",
        "    Args:\n",
        "      x_1: [N, D] float tensor.\n",
        "      y: [N, D] float tensor.\n",
        "      x_2: [N, D] float tensor.\n",
        "      z: [N, D] float tensor.\n",
        "      loss_type: margin or hamming.\n",
        "      margin: float scalar, margin for the margin loss.\n",
        "    Returns:\n",
        "      loss: [N] float tensor.  Loss for each pair of representations.\n",
        "    \"\"\"\n",
        "    if loss_type == 'margin':\n",
        "        return torch.relu(margin +\n",
        "                          euclidean_distance(x_1, y) -\n",
        "                          euclidean_distance(x_2, z))\n",
        "    elif loss_type == 'hamming':\n",
        "        return 0.125 * ((approximate_hamming_similarity(x_1, y) - 1) ** 2 +\n",
        "                        (approximate_hamming_similarity(x_2, z) + 1) ** 2)\n",
        "    else:\n",
        "        raise ValueError('Unknown loss_type %s' % loss_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2X_gKeooJrZ"
      },
      "source": [
        "## Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPOGj9EyoUWd"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "import collections\n",
        "\n",
        "\"\"\"A general Interface\"\"\"\n",
        "\n",
        "GraphData = collections.namedtuple('GraphData', [\n",
        "    'from_idx',\n",
        "    'to_idx',\n",
        "    'node_features',\n",
        "    'edge_features',\n",
        "    'graph_idx',\n",
        "    'n_graphs'])\n",
        "\n",
        "class GraphSimilarityDataset(object):\n",
        "    \"\"\"Base class for all the graph similarity learning datasets.\n",
        "  This class defines some common interfaces a graph similarity dataset can have,\n",
        "  in particular the functions that creates iterators over pairs and triplets.\n",
        "  \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def triplets(self, batch_size):\n",
        "        \"\"\"Create an iterator over triplets.\n",
        "    Args:\n",
        "      batch_size: int, number of triplets in a batch.\n",
        "    Yields:\n",
        "      graphs: a `GraphData` instance.  The batch of triplets put together.  Each\n",
        "        triplet has 3 graphs (x, y, z).  Here the first graph is duplicated once\n",
        "        so the graphs for each triplet are ordered as (x, y, x, z) in the batch.\n",
        "        The batch contains `batch_size` number of triplets, hence `4*batch_size`\n",
        "        many graphs.\n",
        "    \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def pairs(self, batch_size):\n",
        "        \"\"\"Create an iterator over pairs.\n",
        "    Args:\n",
        "      batch_size: int, number of pairs in a batch.\n",
        "    Yields:\n",
        "      graphs: a `GraphData` instance.  The batch of pairs put together.  Each\n",
        "        pair has 2 graphs (x, y).  The batch contains `batch_size` number of\n",
        "        pairs, hence `2*batch_size` many graphs.\n",
        "      labels: [batch_size] int labels for each pair, +1 for similar, -1 for not.\n",
        "    \"\"\"\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IltQvLVrpE_A"
      },
      "source": [
        "## Graph edit distance task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly3b5xSEpJRP"
      },
      "source": [
        "### Graph manipulation primitives\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9TxYZZfpF0o"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "\"\"\"Graph Edit Distance Task\"\"\"\n",
        "\n",
        "\n",
        "# Graph Manipulation Functions\n",
        "def permute_graph_nodes(g):\n",
        "    \"\"\"Permute node ordering of a graph, returns a new graph.\"\"\"\n",
        "    n = g.number_of_nodes()\n",
        "    new_g = nx.Graph()\n",
        "    new_g.add_nodes_from(range(n))\n",
        "    perm = np.random.permutation(n)\n",
        "    edges = g.edges()\n",
        "    new_edges = []\n",
        "    for x, y in edges:\n",
        "        new_edges.append((perm[x], perm[y]))\n",
        "    new_g.add_edges_from(new_edges)\n",
        "    return new_g\n",
        "\n",
        "\n",
        "def substitute_random_edges(g, n):\n",
        "    \"\"\"Substitutes n edges from graph g with another n randomly picked edges.\"\"\"\n",
        "    g = copy.deepcopy(g)\n",
        "    n_nodes = g.number_of_nodes()\n",
        "    edges = list(g.edges())\n",
        "    # sample n edges without replacement\n",
        "    e_remove = [\n",
        "        edges[i] for i in np.random.choice(np.arange(len(edges)), n, replace=False)\n",
        "    ]\n",
        "    edge_set = set(edges)\n",
        "    e_add = set()\n",
        "    while len(e_add) < n:\n",
        "        e = np.random.choice(n_nodes, 2, replace=False)\n",
        "        # make sure e does not exist and is not already chosen to be added\n",
        "        if (\n",
        "                (e[0], e[1]) not in edge_set\n",
        "                and (e[1], e[0]) not in edge_set\n",
        "                and (e[0], e[1]) not in e_add\n",
        "                and (e[1], e[0]) not in e_add\n",
        "        ):\n",
        "            e_add.add((e[0], e[1]))\n",
        "\n",
        "    for i, j in e_remove:\n",
        "        g.remove_edge(i, j)\n",
        "    for i, j in e_add:\n",
        "        g.add_edge(i, j)\n",
        "    return g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8vm0RQQpuiy"
      },
      "source": [
        "### Dataset for training, fixed dataset for evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BQLB7Ftp64w"
      },
      "outputs": [],
      "source": [
        "import contextlib\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class GraphEditDistanceDataset(GraphSimilarityDataset):\n",
        "    \"\"\"Graph edit distance dataset.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_nodes_range,\n",
        "            p_edge_range,\n",
        "            n_changes_positive,\n",
        "            n_changes_negative,\n",
        "            permute=True,\n",
        "    ):\n",
        "        \"\"\"Constructor.\n",
        "    Args:\n",
        "      n_nodes_range: a tuple (n_min, n_max).  The minimum and maximum number of\n",
        "        nodes in a graph to generate.\n",
        "      p_edge_range: a tuple (p_min, p_max).  The minimum and maximum edge\n",
        "        probability.\n",
        "      n_changes_positive: the number of edge substitutions for a pair to be\n",
        "        considered positive (similar).\n",
        "      n_changes_negative: the number of edge substitutions for a pair to be\n",
        "        considered negative (not similar).\n",
        "      permute: if True (default), permute node orderings in addition to\n",
        "        changing edges; if False, the node orderings across a pair or triplet of\n",
        "        graphs will be the same, useful for visualization.\n",
        "    \"\"\"\n",
        "        self._n_min, self._n_max = n_nodes_range\n",
        "        self._p_min, self._p_max = p_edge_range\n",
        "        self._k_pos = n_changes_positive\n",
        "        self._k_neg = n_changes_negative\n",
        "        self._permute = permute\n",
        "\n",
        "    def _get_graph(self):\n",
        "        \"\"\"Generate one graph.\"\"\"\n",
        "        n_nodes = np.random.randint(self._n_min, self._n_max + 1)\n",
        "        p_edge = np.random.uniform(self._p_min, self._p_max)\n",
        "\n",
        "        # do a little bit of filtering\n",
        "        n_trials = 100\n",
        "        for _ in range(n_trials):\n",
        "            g = nx.erdos_renyi_graph(n_nodes, p_edge)\n",
        "            if nx.is_connected(g):\n",
        "                return g\n",
        "\n",
        "        raise ValueError(\"Failed to generate a connected graph.\")\n",
        "\n",
        "    def _get_pair(self, positive):\n",
        "        \"\"\"Generate one pair of graphs.\"\"\"\n",
        "        g = self._get_graph()\n",
        "        if self._permute:\n",
        "            permuted_g = permute_graph_nodes(g)\n",
        "        else:\n",
        "            permuted_g = g\n",
        "        n_changes = self._k_pos if positive else self._k_neg\n",
        "        changed_g = substitute_random_edges(g, n_changes)\n",
        "        return permuted_g, changed_g\n",
        "\n",
        "    def _get_triplet(self):\n",
        "        \"\"\"Generate one triplet of graphs.\"\"\"\n",
        "        g = self._get_graph()\n",
        "        if self._permute:\n",
        "            permuted_g = permute_graph_nodes(g)\n",
        "        else:\n",
        "            permuted_g = g\n",
        "        pos_g = substitute_random_edges(g, self._k_pos)\n",
        "        neg_g = substitute_random_edges(g, self._k_neg)\n",
        "        return permuted_g, pos_g, neg_g\n",
        "\n",
        "    def triplets(self, batch_size):\n",
        "        \"\"\"Yields batches of triplet data.\"\"\"\n",
        "        while True:\n",
        "            batch_graphs = []\n",
        "            for _ in range(batch_size):\n",
        "                g1, g2, g3 = self._get_triplet()\n",
        "                batch_graphs.append((g1, g2, g1, g3))\n",
        "            yield self._pack_batch(batch_graphs)\n",
        "\n",
        "    def pairs(self, batch_size):\n",
        "        \"\"\"Yields batches of pair data.\"\"\"\n",
        "        while True:\n",
        "            batch_graphs = []\n",
        "            batch_labels = []\n",
        "            positive = True\n",
        "            for _ in range(batch_size):\n",
        "                g1, g2 = self._get_pair(positive)\n",
        "                batch_graphs.append((g1, g2))\n",
        "                batch_labels.append(1 if positive else -1)\n",
        "                positive = not positive\n",
        "\n",
        "            packed_graphs = self._pack_batch(batch_graphs)\n",
        "            labels = np.array(batch_labels, dtype=np.int32)\n",
        "            yield packed_graphs, labels\n",
        "\n",
        "    def _pack_batch(self, graphs):\n",
        "        \"\"\"Pack a batch of graphs into a single `GraphData` instance.\n",
        "    Args:\n",
        "      graphs: a list of generated networkx graphs.\n",
        "    Returns:\n",
        "      graph_data: a `GraphData` instance, with node and edge indices properly\n",
        "        shifted.\n",
        "    \"\"\"\n",
        "        Graphs = []\n",
        "        for graph in graphs:\n",
        "            for inergraph in graph:\n",
        "                Graphs.append(inergraph)\n",
        "        graphs = Graphs\n",
        "        from_idx = []\n",
        "        to_idx = []\n",
        "        graph_idx = []\n",
        "\n",
        "        n_total_nodes = 0\n",
        "        n_total_edges = 0\n",
        "        for i, g in enumerate(graphs):\n",
        "            n_nodes = g.number_of_nodes()\n",
        "            n_edges = g.number_of_edges()\n",
        "            edges = np.array(g.edges(), dtype=np.int32)\n",
        "            # shift the node indices for the edges\n",
        "            from_idx.append(edges[:, 0] + n_total_nodes)\n",
        "            to_idx.append(edges[:, 1] + n_total_nodes)\n",
        "            graph_idx.append(np.ones(n_nodes, dtype=np.int32) * i)\n",
        "\n",
        "            n_total_nodes += n_nodes\n",
        "            n_total_edges += n_edges\n",
        "\n",
        "        GraphData = collections.namedtuple('GraphData', [\n",
        "            'from_idx',\n",
        "            'to_idx',\n",
        "            'node_features',\n",
        "            'edge_features',\n",
        "            'graph_idx',\n",
        "            'n_graphs'])\n",
        "\n",
        "        return GraphData(\n",
        "            from_idx=np.concatenate(from_idx, axis=0),\n",
        "            to_idx=np.concatenate(to_idx, axis=0),\n",
        "            # this task only cares about the structures, the graphs have no features.\n",
        "            # setting higher dimension of ones to confirm code functioning\n",
        "            # with high dimensional features.\n",
        "            node_features=np.ones((n_total_nodes, 8), dtype=np.float32),\n",
        "            edge_features=np.ones((n_total_edges, 4), dtype=np.float32),\n",
        "            graph_idx=np.concatenate(graph_idx, axis=0),\n",
        "            n_graphs=len(graphs),\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6HksqE_Ykeo"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Use Fixed datasets for evaluation\n",
        "@contextlib.contextmanager\n",
        "def reset_random_state(seed):\n",
        "    \"\"\"This function creates a context that uses the given seed.\"\"\"\n",
        "    np_rnd_state = np.random.get_state()\n",
        "    rnd_state = random.getstate()\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed + 1)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        random.setstate(rnd_state)\n",
        "        np.random.set_state(np_rnd_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ELDxhPGYnvz"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class FixedGraphEditDistanceDataset(GraphEditDistanceDataset):\n",
        "    \"\"\"A fixed dataset of pairs or triplets for the graph edit distance task.\n",
        "  This dataset can be used for evaluation.\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_nodes_range,\n",
        "            p_edge_range,\n",
        "            n_changes_positive,\n",
        "            n_changes_negative,\n",
        "            dataset_size,\n",
        "            permute=True,\n",
        "            seed=1234,\n",
        "    ):\n",
        "        super(FixedGraphEditDistanceDataset, self).__init__(\n",
        "            n_nodes_range,\n",
        "            p_edge_range,\n",
        "            n_changes_positive,\n",
        "            n_changes_negative,\n",
        "            permute=permute,\n",
        "        )\n",
        "        self._dataset_size = dataset_size\n",
        "        self._seed = seed\n",
        "\n",
        "    def triplets(self, batch_size):\n",
        "        \"\"\"Yield triplets.\"\"\"\n",
        "\n",
        "        if hasattr(self, \"_triplets\"):\n",
        "            triplets = self._triplets\n",
        "        else:\n",
        "            # get a fixed set of triplets\n",
        "            with reset_random_state(self._seed):\n",
        "                triplets = []\n",
        "                for _ in range(self._dataset_size):\n",
        "                    g1, g2, g3 = self._get_triplet()\n",
        "                    triplets.append((g1, g2, g1, g3))\n",
        "            self._triplets = triplets\n",
        "\n",
        "        ptr = 0\n",
        "        while ptr + batch_size <= len(triplets):\n",
        "            batch_graphs = triplets[ptr: ptr + batch_size]\n",
        "            yield self._pack_batch(batch_graphs)\n",
        "            ptr += batch_size\n",
        "\n",
        "    def pairs(self, batch_size):\n",
        "        \"\"\"Yield pairs and labels.\"\"\"\n",
        "\n",
        "        if hasattr(self, \"_pairs\") and hasattr(self, \"_labels\"):\n",
        "            pairs = self._pairs\n",
        "            labels = self._labels\n",
        "        else:\n",
        "            # get a fixed set of pairs first\n",
        "            with reset_random_state(self._seed):\n",
        "                pairs = []\n",
        "                labels = []\n",
        "                positive = True\n",
        "                for _ in range(self._dataset_size):\n",
        "                    pairs.append(self._get_pair(positive))\n",
        "                    labels.append(1 if positive else -1)\n",
        "                    positive = not positive\n",
        "            labels = np.array(labels, dtype=np.int32)\n",
        "\n",
        "            self._pairs = pairs\n",
        "            self._labels = labels\n",
        "\n",
        "        ptr = 0\n",
        "        while ptr + batch_size <= len(pairs):\n",
        "            batch_graphs = pairs[ptr: ptr + batch_size]\n",
        "            packed_batch = self._pack_batch(batch_graphs)\n",
        "            yield packed_batch, labels[ptr: ptr + batch_size]\n",
        "            ptr += batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMzgtdt_4jwf"
      },
      "source": [
        "###QM7b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7XRXxhxUEry",
        "outputId": "18313829-2219-4a09-f71c-8e69bbc23f59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.4 MB 4.3 MB/s \n",
            "\u001b[?25hSetting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading /root/.dgl/qm7b.mat from http://deepchem.io.s3-website-us-west-1.amazonaws.com/datasets/qm7b.mat...\n"
          ]
        }
      ],
      "source": [
        "!pip install dgl -q\n",
        "from dgl.data import QM7bDataset\n",
        "qm = QM7bDataset()\n",
        "\n",
        "\n",
        "class QM7b():\n",
        "  #只截取前20个图  177 个节点\t 1713 个边 生成的微调图会删去两条边 3386个边 删去的边而影响的节点？？？？354个节点\n",
        "  def pairs(self,batch_size):#读前20个图微调生成另外20个，打包成namedtuple\n",
        "    while True:\n",
        "      batch_graphs = []\n",
        "      batch_labels = []\n",
        "      positive = True \n",
        "      for i in range(batch_size):#默认20\n",
        "        g1,_ = qm[i]\n",
        "        g2 = g1.clone()\n",
        "        if positive:#True  移除2个from_edge_idx\n",
        "          remove_idx = np.random.choice(np.arange(g2.num_edges()),1,replace=False)\n",
        "        else:#False 移除3个from_edge_idx\n",
        "          remove_idx = np.random.choice(np.arange(g2.num_edges()),2,replace=False)\n",
        "        g2.remove_edges(remove_idx)\n",
        "        batch_graphs.append((g1, g2))#list里添20个tuple\n",
        "        batch_labels.append(1 if positive else -1)#list里20个整数\n",
        "        positive = not positive#一正一负\n",
        "      labels = np.array(batch_labels, dtype=np.int32)\n",
        "      from_idx = []\n",
        "      to_idx = []\n",
        "      graph_idx = []\n",
        "      graphs = []\n",
        "      num_nodes = 0\n",
        "      num_edges = 0\n",
        "      for tuples in batch_graphs:#拆list 20个元祖\n",
        "        for pair in tuples:#拆元组 2个图\n",
        "          graphs.append(pair)#list里40个图\n",
        "      batch_graphs = graphs\n",
        "      for i,g in enumerate(batch_graphs):#返回的是索引以及索引对应元素\n",
        "        edge_idx = torch.arange(0,g.num_edges())\n",
        "        src, dst = g.find_edges(edge_idx)\n",
        "        src = np.array(src, dtype=np.int32)\n",
        "        dst = np.array(dst, dtype=np.int32)\n",
        "        from_idx.append(src)\n",
        "        to_idx.append(dst)\n",
        "        num_nodes += g.num_nodes()\n",
        "        num_edges += g.num_edges()\n",
        "        graph_idx.append(np.ones(g.num_nodes(), dtype=np.int32) * i)\n",
        "      GraphData = collections.namedtuple('GraphData', ['from_idx','to_idx','node_features','edge_features', 'graph_idx','n_graphs'])\n",
        "      packed_graphs = GraphData(\n",
        "              from_idx=np.concatenate(from_idx, axis=0),\n",
        "              to_idx=np.concatenate(to_idx, axis=0),\n",
        "              node_features=np.ones((num_nodes, 8), dtype=np.float32),\n",
        "              edge_features=np.ones((num_edges, 4), dtype=np.float32),\n",
        "              graph_idx=np.concatenate(graph_idx, axis=0),\n",
        "              n_graphs=len(batch_graphs),\n",
        "      )\n",
        "      yield packed_graphs, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-jZ1VIvT3cJ"
      },
      "outputs": [],
      "source": [
        "class QM7bVali():\n",
        "  def pairs(self,batch_size):\n",
        "    batch_graphs = []\n",
        "    batch_labels = []\n",
        "    positive = True \n",
        "    for i in range(1000):#默认20还是1000\n",
        "      g1,_ = qm[i+20]\n",
        "      g2 = g1.clone()\n",
        "      if positive:#True  移除2个from_edge_idx\n",
        "        remove_idx =  np.random.choice(np.arange(g2.num_edges()),1,replace=False)\n",
        "      else:#False 移除3个from_edge_idx\n",
        "        remove_idx = np.random.choice(np.arange(g2.num_edges()),5,replace=False)\n",
        "      g2.remove_edges(remove_idx)\n",
        "      batch_graphs.append((g1, g2))#list里添20个tuple\n",
        "      batch_labels.append(1 if positive else -1)#list里20个整数\n",
        "      positive = not positive#一正一负\n",
        "    labels = np.array(batch_labels, dtype=np.int32)\n",
        "\n",
        "    ptr = 0\n",
        "    while ptr + batch_size <= len(batch_graphs):\n",
        "      batch_graphs = batch_graphs[ptr: ptr + batch_size]\n",
        "      from_idx = []\n",
        "      to_idx = []\n",
        "      graph_idx = []\n",
        "      graphs = []\n",
        "      num_nodes = 0\n",
        "      num_edges = 0\n",
        "      for tuples in batch_graphs:#拆list 20个元祖\n",
        "        for pair in tuples:#拆元组 2个图\n",
        "          graphs.append(pair)#list里40个图\n",
        "      for i,g in enumerate(graphs):#返回的是索引以及索引对应元素\n",
        "        edge_idx = torch.arange(0,g.num_edges())\n",
        "        src, dst = g.find_edges(edge_idx)\n",
        "        src = np.array(src, dtype=np.int32)\n",
        "        dst = np.array(dst, dtype=np.int32)\n",
        "        from_idx.append(src)\n",
        "        to_idx.append(dst)\n",
        "        num_nodes += g.num_nodes()\n",
        "        num_edges += g.num_edges()\n",
        "        graph_idx.append(np.ones(g.num_nodes(), dtype=np.int32) * i)\n",
        "      GraphData = collections.namedtuple('GraphData', ['from_idx','to_idx','node_features','edge_features', 'graph_idx','n_graphs'])\n",
        "      packed_graphs = GraphData(\n",
        "              from_idx=np.concatenate(from_idx, axis=0),\n",
        "              to_idx=np.concatenate(to_idx, axis=0),\n",
        "              node_features=np.ones((num_nodes, 8), dtype=np.float32),\n",
        "              edge_features=np.ones((num_edges, 4), dtype=np.float32),\n",
        "              graph_idx=np.concatenate(graph_idx, axis=0),\n",
        "              n_graphs=len(graphs),\n",
        "      )\n",
        "      yield packed_graphs, labels[ptr: ptr + batch_size]\n",
        "      ptr += batch_size\n",
        "\n",
        "  def triplets(self, batch_size):\n",
        "        \n",
        "    triplets = []\n",
        "    for i in range(1000):\n",
        "        g1,_ = qm[i+20]\n",
        "        g2 = g1.clone()\n",
        "        g3 = g1.clone()\n",
        "        remove_idx =  np.random.choice(np.arange(g2.num_edges()),1,replace=False)\n",
        "        g2.remove_edges(remove_idx)\n",
        "        remove_idx =  np.random.choice(np.arange(g2.num_edges()),5,replace=False)\n",
        "        g3.remove_edges(remove_idx)\n",
        "        triplets.append((g1, g2, g1, g3))\n",
        "\n",
        "    ptr = 0\n",
        "    while ptr + batch_size <= len(triplets):\n",
        "      batch_graphs = triplets[ptr: ptr + batch_size]\n",
        "\n",
        "      from_idx = []\n",
        "      to_idx = []\n",
        "      graph_idx = []\n",
        "      graphs = []\n",
        "      num_nodes = 0\n",
        "      num_edges = 0\n",
        "      for tuples in batch_graphs:#拆list 20个元祖\n",
        "        for pair in tuples:#拆元组 2个图\n",
        "          graphs.append(pair)#list里40个图\n",
        "      for i,g in enumerate(graphs):#返回的是索引以及索引对应元素\n",
        "        edge_idx = torch.arange(0,g.num_edges())\n",
        "        src, dst = g.find_edges(edge_idx)\n",
        "        src = np.array(src, dtype=np.int32)\n",
        "        dst = np.array(dst, dtype=np.int32)\n",
        "        from_idx.append(src)\n",
        "        to_idx.append(dst)\n",
        "        num_nodes += g.num_nodes()\n",
        "        num_edges += g.num_edges()\n",
        "        graph_idx.append(np.ones(g.num_nodes(), dtype=np.int32) * i)\n",
        "      GraphData = collections.namedtuple('GraphData', ['from_idx','to_idx','node_features','edge_features', 'graph_idx','n_graphs'])\n",
        "      packed_graphs = GraphData(\n",
        "              from_idx=np.concatenate(from_idx, axis=0),\n",
        "              to_idx=np.concatenate(to_idx, axis=0),\n",
        "              node_features=np.ones((num_nodes, 8), dtype=np.float32),\n",
        "              edge_features=np.ones((num_edges, 4), dtype=np.float32),\n",
        "              graph_idx=np.concatenate(graph_idx, axis=0),\n",
        "              n_graphs=len(graphs),\n",
        "      )\n",
        "      yield packed_graphs\n",
        "      ptr += batch_size\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaYUL4aHqNOH"
      },
      "source": [
        "## Building the model, and the training and evaluation pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OZoIFRUqUw1"
      },
      "source": [
        "### Configs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNISCW4tqXhl"
      },
      "outputs": [],
      "source": [
        "def get_default_config():\n",
        "    \"\"\"The default configs.\"\"\"\n",
        "    model_type = 'matching'  #如果用嵌入网络就设置为 `embedding\n",
        "    # Set to `embedding` to use the graph embedding net.\n",
        "    node_state_dim = 32\n",
        "    edge_state_dim = 16\n",
        "    graph_rep_dim = 128\n",
        "    graph_embedding_net_config = dict(\n",
        "        node_state_dim=node_state_dim,\n",
        "        edge_state_dim=edge_state_dim,\n",
        "        edge_hidden_sizes=[node_state_dim * 2, node_state_dim * 2],\n",
        "        node_hidden_sizes=[node_state_dim * 2],\n",
        "        n_prop_layers=5,\n",
        "        # set to False to not share parameters across message passing layers\n",
        "        share_prop_params=True,   # 判断在信息传递层是否参数共享\n",
        "        # initialize message MLP with small parameter weights to prevent\n",
        "        # aggregated message vectors blowing up, alternatively we could also use\n",
        "        # e.g. layer normalization to keep the scale of these under control.\n",
        "        edge_net_init_scale=0.1,#用小参数权重初始化消息 MLP 以防止聚合消息向量爆炸\n",
        "\t                              #或者也可以使用例如 层标准化以控制这些的规模。\n",
        "        # other types of update like `mlp` and `residual` can also be used here. gru\n",
        "        node_update_type='gru', # 其他也可以用'mlp'  `residual`\n",
        "        # set to False if your graph already contains edges in both directions.\n",
        "        use_reverse_direction=True,     #如果有双向边就设成FALSE\n",
        "        # set to True if your graph is directed\n",
        "        reverse_dir_param_different=False,  #如果是有向图则设成TRUE\n",
        "        # we didn't use layer norm in our experiments but sometimes this can help.\n",
        "        layer_norm=False,# 在实验中没有使用层范数，但有时会有用\n",
        "        # set to `embedding` to use the graph embedding net.\n",
        "        prop_type=model_type)\n",
        "    graph_matching_net_config = graph_embedding_net_config.copy()\n",
        "    graph_matching_net_config['similarity'] = 'dotproduct'  # other: euclidean, cosine\n",
        "    return dict(\n",
        "        encoder=dict(\n",
        "            node_hidden_sizes=[node_state_dim],\n",
        "            node_feature_dim=1,\n",
        "            edge_hidden_sizes=[edge_state_dim]),\n",
        "        aggregator=dict(\n",
        "            node_hidden_sizes=[graph_rep_dim],\n",
        "            graph_transform_sizes=[graph_rep_dim],\n",
        "            input_size=[node_state_dim],\n",
        "            gated=True,\n",
        "            aggregation_type='sum'),\n",
        "        graph_embedding_net=graph_embedding_net_config,\n",
        "        graph_matching_net=graph_matching_net_config,\n",
        "        model_type=model_type,\n",
        "        data=dict(\n",
        "            problem='graph_edit_distance',\n",
        "            dataset_params=dict(\n",
        "                # always generate graphs with 20 nodes and p_edge=0.2.\n",
        "                n_nodes_range=[20, 20],\n",
        "                p_edge_range=[0.2, 0.2],#生成具有 20 个节点且 p_edge=0.2 （边概率？）的图\n",
        "                n_changes_positive=1,  #一对被视为正（相似）的边替换的数量\n",
        "                n_changes_negative=2,\n",
        "                validation_dataset_size=1000)),\n",
        "        training=dict(\n",
        "            batch_size=20,\n",
        "            learning_rate=1e-4,\n",
        "            mode='pair',\n",
        "            loss='margin',  # other: hamming\n",
        "            margin=1.0,\n",
        "            # A small regularizer on the graph vector scales to avoid the graph\n",
        "            # vectors blowing up.  If numerical issues is particularly bad in the\n",
        "            # model we can add `snt.LayerNorm` to the outputs of each layer, the\n",
        "            # aggregated messages and aggregated node representations to\n",
        "            # keep the network activation scale in a reasonable range.\n",
        "            graph_vec_regularizer_weight=1e-6,\t  #图向量上有一个小的正则化器会缩放以避免图向量爆炸。 \n",
        "                                                  #如果模型中的数值问题特别严重，\n",
        "                                                  #可以将 `snt.LayerNorm` 添加到每一层的输出、聚合消息和聚合节点表示中，\n",
        "                                                  #以将网络激活规模保持在合理范围内。\n",
        "            # Add gradient clipping to avoid large gradients.\n",
        "            clip_value=10.0,   #设置梯度裁剪防止梯度爆炸\n",
        "            # '''\n",
        "            # 梯度爆炸：在神经网络中，当前面隐藏层的学习速率低于后面隐藏层的学习速率，\n",
        "            # 即随着隐藏层数目的增加，分类准确率反而下降了。 这种现象叫梯度爆炸。 \n",
        "            # 其实梯度消失和梯度爆炸是一回事，只是表现的形式，以及产生的原因不一样。\n",
        "            # '''\n",
        "            # Increase this to train longer.\n",
        "            n_training_steps=500000,   #控制训练时长\n",
        "            # Print training information every this many training steps.\n",
        "            print_after=100,   #每隔这么多训练步骤打印训练信息\n",
        "            # Evaluate on validation set every `eval_after * print_after` steps.\n",
        "            eval_after=10),    #每个 `eval_after * print_after` 步骤对验证集进行评估。\n",
        "        evaluation=dict(\n",
        "            batch_size=20),\n",
        "        seed=8,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRCzsNGgrL6b"
      },
      "source": [
        "### Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEjEnIFjrPJh"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def exact_hamming_similarity(x, y):\n",
        "    \"\"\"Compute the binary Hamming similarity.\"\"\"\n",
        "    match = ((x > 0) * (y > 0)).float()\n",
        "    return torch.mean(match, dim=1)\n",
        "\n",
        "\n",
        "def compute_similarity(config, x, y):\n",
        "    \"\"\"Compute the distance between x and y vectors.\n",
        "    The distance will be computed based on the training loss type.\n",
        "    Args:\n",
        "      config: a config dict.\n",
        "      x: [n_examples, feature_dim] float tensor.\n",
        "      y: [n_examples, feature_dim] float tensor.\n",
        "    Returns:\n",
        "      dist: [n_examples] float tensor.\n",
        "    Raises:\n",
        "      ValueError: if loss type is not supported.\n",
        "    \"\"\"\n",
        "    if config['training']['loss'] == 'margin':\n",
        "        # similarity is negative distance\n",
        "        return -euclidean_distance(x, y)\n",
        "    elif config['training']['loss'] == 'hamming':\n",
        "        return exact_hamming_similarity(x, y)\n",
        "    else:\n",
        "        raise ValueError('Unknown loss type %s' % config['training']['loss'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnozmrSK9zz1"
      },
      "outputs": [],
      "source": [
        "def auc(scores, labels, **auc_args):\n",
        "    \"\"\"Compute the AUC for pair classification.\n",
        "    See `tf.metrics.auc` for more details about this metric.\n",
        "    Args:\n",
        "      scores: [n_examples] float.  Higher scores mean higher preference of being\n",
        "        assigned the label of +1.\n",
        "      labels: [n_examples] int.  Labels are either +1 or -1.\n",
        "      **auc_args: other arguments that can be used by `tf.metrics.auc`.\n",
        "    Returns:\n",
        "      auc: the area under the ROC curve.\n",
        "    \"\"\"\n",
        "    scores_max = torch.max(scores)\n",
        "    scores_min = torch.min(scores)\n",
        "\n",
        "    # normalize scores to [0, 1] and add a small epislon for safety\n",
        "    scores = (scores - scores_min) / (scores_max - scores_min + 1e-8)\n",
        "\n",
        "    labels = (labels + 1) / 2\n",
        "\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(labels.cpu().detach().numpy(), scores.cpu().detach().numpy())\n",
        "    return metrics.auc(fpr, tpr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCzglLcrsTRO"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v6wXab5skO1"
      },
      "outputs": [],
      "source": [
        "def reshape_and_split_tensor(tensor, n_splits):\n",
        "    \"\"\"Reshape and split a 2D tensor along the last dimension.\n",
        "    Args:\n",
        "      tensor: a [num_examples, feature_dim] tensor.  num_examples must be a\n",
        "        multiple of `n_splits`.\n",
        "      n_splits: int, number of splits to split the tensor into.\n",
        "    Returns:\n",
        "      splits: a list of `n_splits` tensors.  The first split is [tensor[0],\n",
        "        tensor[n_splits], tensor[n_splits * 2], ...], the second split is\n",
        "        [tensor[1], tensor[n_splits + 1], tensor[n_splits * 2 + 1], ...], etc..\n",
        "    \"\"\"\n",
        "    feature_dim = tensor.shape[-1]\n",
        "    tensor = torch.reshape(tensor, [-1, feature_dim * n_splits])\n",
        "    tensor_split = []\n",
        "    for i in range(n_splits):\n",
        "        tensor_split.append(tensor[:, feature_dim * i: feature_dim * (i + 1)])\n",
        "    return tensor_split\n",
        "\n",
        "\n",
        "def build_model(config, node_feature_dim, edge_feature_dim):\n",
        "    \"\"\"Create model for training and evaluation.\n",
        "    Args:\n",
        "      config: a dictionary of configs, like the one created by the\n",
        "        `get_default_config` function.\n",
        "      node_feature_dim: int, dimensionality of node features.\n",
        "      edge_feature_dim: int, dimensionality of edge features.\n",
        "    Returns:\n",
        "      tensors: a (potentially nested) name => tensor dict.\n",
        "      placeholders: a (potentially nested) name => tensor dict.\n",
        "      AE_model: a GraphEmbeddingNet or GraphMatchingNet instance.\n",
        "    Raises:\n",
        "      ValueError: if the specified model or training settings are not supported.\n",
        "    \"\"\"\n",
        "    config['encoder']['node_feature_dim'] = node_feature_dim\n",
        "    config['encoder']['edge_feature_dim'] = edge_feature_dim\n",
        "\n",
        "    encoder = GraphEncoder(**config['encoder'])\n",
        "    aggregator = GraphAggregator(**config['aggregator'])\n",
        "    if config['model_type'] == 'embedding':\n",
        "        model = GraphEmbeddingNet(\n",
        "            encoder, aggregator, **config['graph_embedding_net'])\n",
        "    elif config['model_type'] == 'matching':\n",
        "        model = GraphMatchingNet(\n",
        "            encoder, aggregator, **config['graph_matching_net'])\n",
        "    else:\n",
        "        raise ValueError('Unknown model type: %s' % config['model_type'])\n",
        "\n",
        "    optimizer = torch.optim.Adam((model.parameters()),\n",
        "                                 lr=config['training']['learning_rate'], weight_decay=1e-5)\n",
        "\n",
        "    return model, optimizer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ql9O0GklECb"
      },
      "source": [
        "### build the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EDI9QvylWBj"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "def build_datasets(config):\n",
        "    \"\"\"Build the training and evaluation datasets.\"\"\"\n",
        "    config = copy.deepcopy(config)\n",
        "\n",
        "    if config['data']['problem'] == 'graph_edit_distance':\n",
        "        dataset_params = config['data']['dataset_params']\n",
        "        validation_dataset_size = dataset_params['validation_dataset_size']\n",
        "        del dataset_params['validation_dataset_size']\n",
        "        training_set = GraphEditDistanceDataset(**dataset_params)\n",
        "        dataset_params['dataset_size'] = validation_dataset_size\n",
        "        validation_set = FixedGraphEditDistanceDataset(**dataset_params)\n",
        "    elif config['data']['problem'] == 'QM7b':\n",
        "      training_set = QM7b()\n",
        "      validation_set = QM7bVali()\n",
        "    else:\n",
        "        raise ValueError('Unknown problem type: %s' % config['data']['problem'])\n",
        "    return training_set, validation_set\n",
        "\n",
        "\n",
        "def get_graph(batch):\n",
        "    if len(batch) != 2:\n",
        "        # if isinstance(batch, GraphData):\n",
        "        graph = batch\n",
        "        node_features = torch.from_numpy(graph.node_features)\n",
        "        edge_features = torch.from_numpy(graph.edge_features)\n",
        "        from_idx = torch.from_numpy(graph.from_idx).long()\n",
        "        to_idx = torch.from_numpy(graph.to_idx).long()\n",
        "        graph_idx = torch.from_numpy(graph.graph_idx).long()\n",
        "        return node_features, edge_features, from_idx, to_idx, graph_idx\n",
        "    else:\n",
        "        graph, labels = batch\n",
        "        node_features = torch.from_numpy(graph.node_features)\n",
        "        edge_features = torch.from_numpy(graph.edge_features)\n",
        "        from_idx = torch.from_numpy(graph.from_idx).long()\n",
        "        to_idx = torch.from_numpy(graph.to_idx).long()\n",
        "        graph_idx = torch.from_numpy(graph.graph_idx).long()\n",
        "        labels = torch.from_numpy(labels).long()\n",
        "    return node_features, edge_features, from_idx, to_idx, graph_idx, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxMjDskali9m"
      },
      "source": [
        "### Let's run it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j81erASg6ge",
        "outputId": "e556d180-5a08-44f9-c8a7-226a384824dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoder= {'node_hidden_sizes': [32], 'node_feature_dim': 1, 'edge_hidden_sizes': [16]}\n",
            "aggregator= {'node_hidden_sizes': [128], 'graph_transform_sizes': [128], 'input_size': [32], 'gated': True, 'aggregation_type': 'sum'}\n",
            "graph_embedding_net= {'node_state_dim': 32, 'edge_state_dim': 16, 'edge_hidden_sizes': [64, 64], 'node_hidden_sizes': [64], 'n_prop_layers': 5, 'share_prop_params': True, 'edge_net_init_scale': 0.1, 'node_update_type': 'gru', 'use_reverse_direction': True, 'reverse_dir_param_different': False, 'layer_norm': False, 'prop_type': 'matching'}\n",
            "graph_matching_net= {'node_state_dim': 32, 'edge_state_dim': 16, 'edge_hidden_sizes': [64, 64], 'node_hidden_sizes': [64], 'n_prop_layers': 5, 'share_prop_params': True, 'edge_net_init_scale': 0.1, 'node_update_type': 'gru', 'use_reverse_direction': True, 'reverse_dir_param_different': False, 'layer_norm': False, 'prop_type': 'matching', 'similarity': 'dotproduct'}\n",
            "model_type= matching\n",
            "data= {'problem': 'graph_edit_distance', 'dataset_params': {'n_nodes_range': [20, 20], 'p_edge_range': [0.2, 0.2], 'n_changes_positive': 1, 'n_changes_negative': 2, 'validation_dataset_size': 1000}}\n",
            "training= {'batch_size': 20, 'learning_rate': 0.0001, 'mode': 'pair', 'loss': 'margin', 'margin': 1.0, 'graph_vec_regularizer_weight': 1e-06, 'clip_value': 10.0, 'n_training_steps': 500000, 'print_after': 100, 'eval_after': 10}\n",
            "evaluation= {'batch_size': 20}\n",
            "seed= 8\n",
            "iter 100, loss 0.9829, sim_pos -0.0472, sim_neg -0.0814, sim_diff 0.0342, time 18.46s\n",
            "iter 200, loss 1.0421, sim_pos -0.7152, sim_neg -0.7558, sim_diff 0.0406, time 16.56s\n",
            "iter 300, loss 0.8694, sim_pos -0.3975, sim_neg -0.7833, sim_diff 0.3858, time 16.90s\n",
            "iter 400, loss 0.9134, sim_pos -0.4955, sim_neg -0.8989, sim_diff 0.4034, time 16.98s\n",
            "iter 500, loss 0.7837, sim_pos -0.5371, sim_neg -1.0221, sim_diff 0.4850, time 16.51s\n",
            "iter 600, loss 0.7416, sim_pos -0.7314, sim_neg -1.3598, sim_diff 0.6284, time 16.66s\n",
            "iter 700, loss 0.8756, sim_pos -0.7792, sim_neg -1.4652, sim_diff 0.6860, time 16.54s\n",
            "iter 800, loss 0.8008, sim_pos -0.7716, sim_neg -1.5947, sim_diff 0.8230, time 16.58s\n",
            "iter 900, loss 0.8946, sim_pos -0.4536, sim_neg -0.9882, sim_diff 0.5347, time 16.75s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 1000, loss 1.0307, sim_pos -1.0216, sim_neg -1.1651, sim_diff 0.1434, val/pair_auc 0.6844, val/triplet_acc 0.6910, time 26.29s\n",
            "iter 1100, loss 0.9112, sim_pos -0.7195, sim_neg -0.9730, sim_diff 0.2536, time 16.63s\n",
            "iter 1200, loss 0.7054, sim_pos -0.4615, sim_neg -1.2632, sim_diff 0.8017, time 16.60s\n",
            "iter 1300, loss 1.0608, sim_pos -0.7628, sim_neg -0.6411, sim_diff -0.1217, time 16.73s\n",
            "iter 1400, loss 0.8628, sim_pos -0.6238, sim_neg -0.9948, sim_diff 0.3711, time 16.89s\n",
            "iter 1500, loss 0.6248, sim_pos -0.5409, sim_neg -1.3541, sim_diff 0.8132, time 16.51s\n",
            "iter 1600, loss 0.7941, sim_pos -0.5874, sim_neg -1.1584, sim_diff 0.5710, time 16.56s\n",
            "iter 1700, loss 0.7406, sim_pos -0.7468, sim_neg -1.5664, sim_diff 0.8196, time 16.53s\n",
            "iter 1800, loss 0.7885, sim_pos -0.8307, sim_neg -1.7212, sim_diff 0.8905, time 16.82s\n",
            "iter 1900, loss 0.6148, sim_pos -0.5820, sim_neg -1.7180, sim_diff 1.1360, time 16.44s\n",
            "iter 2000, loss 0.8600, sim_pos -0.7455, sim_neg -1.4791, sim_diff 0.7336, val/pair_auc 0.7180, val/triplet_acc 0.7300, time 23.48s\n",
            "iter 2100, loss 0.8212, sim_pos -0.8075, sim_neg -1.3881, sim_diff 0.5805, time 16.39s\n",
            "iter 2200, loss 0.6080, sim_pos -0.5088, sim_neg -1.5004, sim_diff 0.9916, time 16.43s\n",
            "iter 2300, loss 0.6808, sim_pos -0.7593, sim_neg -1.8090, sim_diff 1.0497, time 16.68s\n",
            "iter 2400, loss 0.7674, sim_pos -0.7407, sim_neg -1.3397, sim_diff 0.5990, time 16.49s\n",
            "iter 2500, loss 0.8261, sim_pos -0.9354, sim_neg -1.7021, sim_diff 0.7666, time 16.44s\n",
            "iter 2600, loss 0.9453, sim_pos -1.0038, sim_neg -1.2605, sim_diff 0.2567, time 16.55s\n",
            "iter 2700, loss 0.6947, sim_pos -0.7143, sim_neg -1.5091, sim_diff 0.7948, time 16.38s\n",
            "iter 2800, loss 0.7603, sim_pos -0.6393, sim_neg -1.2986, sim_diff 0.6592, time 16.80s\n",
            "iter 2900, loss 0.7393, sim_pos -0.6989, sim_neg -1.4381, sim_diff 0.7392, time 16.46s\n",
            "iter 3000, loss 0.6447, sim_pos -0.6924, sim_neg -1.8186, sim_diff 1.1263, val/pair_auc 0.7432, val/triplet_acc 0.7460, time 23.35s\n",
            "iter 3100, loss 0.8420, sim_pos -0.8619, sim_neg -1.6292, sim_diff 0.7673, time 16.44s\n",
            "iter 3200, loss 0.6966, sim_pos -0.5579, sim_neg -1.2309, sim_diff 0.6730, time 16.73s\n",
            "iter 3300, loss 0.5541, sim_pos -0.6789, sim_neg -1.8565, sim_diff 1.1776, time 16.44s\n",
            "iter 3400, loss 0.7377, sim_pos -0.7961, sim_neg -1.5018, sim_diff 0.7057, time 16.44s\n",
            "iter 3500, loss 0.7975, sim_pos -0.8808, sim_neg -1.6599, sim_diff 0.7791, time 16.73s\n",
            "iter 3600, loss 0.7010, sim_pos -0.6693, sim_neg -1.4404, sim_diff 0.7711, time 16.40s\n",
            "iter 3700, loss 0.8736, sim_pos -0.8904, sim_neg -1.3602, sim_diff 0.4697, time 16.44s\n",
            "iter 3800, loss 0.6489, sim_pos -0.7100, sim_neg -1.7651, sim_diff 1.0551, time 16.33s\n",
            "iter 3900, loss 0.5257, sim_pos -0.7972, sim_neg -1.9160, sim_diff 1.1188, time 16.60s\n",
            "iter 4000, loss 0.5287, sim_pos -0.8092, sim_neg -1.8647, sim_diff 1.0555, val/pair_auc 0.7510, val/triplet_acc 0.7450, time 23.33s\n",
            "iter 4100, loss 0.7902, sim_pos -0.6958, sim_neg -1.2269, sim_diff 0.5311, time 16.30s\n",
            "iter 4200, loss 0.7081, sim_pos -0.7139, sim_neg -1.2977, sim_diff 0.5837, time 16.44s\n",
            "iter 4300, loss 0.6942, sim_pos -0.7924, sim_neg -1.5796, sim_diff 0.7872, time 16.59s\n",
            "iter 4400, loss 0.8935, sim_pos -0.9122, sim_neg -1.2739, sim_diff 0.3616, time 16.47s\n",
            "iter 4500, loss 0.8876, sim_pos -0.6973, sim_neg -1.1157, sim_diff 0.4184, time 16.41s\n",
            "iter 4600, loss 0.7167, sim_pos -0.8579, sim_neg -1.5472, sim_diff 0.6894, time 16.35s\n",
            "iter 4700, loss 0.5917, sim_pos -0.6474, sim_neg -1.7930, sim_diff 1.1455, time 16.30s\n",
            "iter 4800, loss 0.7123, sim_pos -0.9171, sim_neg -1.8068, sim_diff 0.8897, time 16.52s\n",
            "iter 4900, loss 0.7661, sim_pos -0.8225, sim_neg -1.3423, sim_diff 0.5198, time 16.31s\n",
            "iter 5000, loss 0.6241, sim_pos -0.7261, sim_neg -1.5316, sim_diff 0.8054, val/pair_auc 0.7598, val/triplet_acc 0.7720, time 23.39s\n",
            "iter 5100, loss 0.9924, sim_pos -1.1725, sim_neg -1.1877, sim_diff 0.0152, time 16.41s\n",
            "iter 5200, loss 0.6573, sim_pos -0.8012, sim_neg -1.5064, sim_diff 0.7052, time 16.39s\n",
            "iter 5300, loss 0.8772, sim_pos -1.2215, sim_neg -1.7224, sim_diff 0.5009, time 16.57s\n",
            "iter 5400, loss 0.8835, sim_pos -0.8052, sim_neg -1.0382, sim_diff 0.2331, time 16.45s\n",
            "iter 5500, loss 0.8045, sim_pos -0.9652, sim_neg -1.6313, sim_diff 0.6661, time 16.42s\n",
            "iter 5600, loss 0.7174, sim_pos -0.7391, sim_neg -1.4958, sim_diff 0.7568, time 16.43s\n",
            "iter 5700, loss 0.7668, sim_pos -0.6628, sim_neg -1.2060, sim_diff 0.5433, time 17.47s\n",
            "iter 5800, loss 0.5417, sim_pos -0.6592, sim_neg -1.7967, sim_diff 1.1375, time 16.68s\n",
            "iter 5900, loss 0.7474, sim_pos -0.9805, sim_neg -1.5937, sim_diff 0.6132, time 16.74s\n",
            "iter 6000, loss 0.7889, sim_pos -1.0190, sim_neg -1.5487, sim_diff 0.5297, val/pair_auc 0.7592, val/triplet_acc 0.7700, time 23.46s\n",
            "iter 6100, loss 0.7051, sim_pos -0.9220, sim_neg -1.5147, sim_diff 0.5927, time 16.34s\n",
            "iter 6200, loss 0.5533, sim_pos -0.7647, sim_neg -2.1204, sim_diff 1.3557, time 16.40s\n",
            "iter 6300, loss 0.6877, sim_pos -0.7663, sim_neg -1.6041, sim_diff 0.8378, time 16.58s\n",
            "iter 6400, loss 0.8088, sim_pos -1.0184, sim_neg -1.5557, sim_diff 0.5372, time 16.54s\n",
            "iter 6500, loss 0.5586, sim_pos -0.7449, sim_neg -2.0420, sim_diff 1.2970, time 16.57s\n",
            "iter 6600, loss 0.6978, sim_pos -0.7647, sim_neg -1.5620, sim_diff 0.7974, time 16.51s\n",
            "iter 6700, loss 0.5240, sim_pos -0.6860, sim_neg -1.6883, sim_diff 1.0023, time 16.69s\n",
            "iter 6800, loss 0.8498, sim_pos -0.9145, sim_neg -1.5904, sim_diff 0.6759, time 16.45s\n",
            "iter 6900, loss 0.7240, sim_pos -0.8123, sim_neg -1.4992, sim_diff 0.6869, time 16.45s\n",
            "iter 7000, loss 0.8379, sim_pos -0.7189, sim_neg -1.1577, sim_diff 0.4388, val/pair_auc 0.7788, val/triplet_acc 0.7920, time 23.54s\n",
            "iter 7100, loss 0.8300, sim_pos -0.9670, sim_neg -1.5042, sim_diff 0.5372, time 16.37s\n",
            "iter 7200, loss 0.5922, sim_pos -0.5790, sim_neg -1.4196, sim_diff 0.8406, time 16.59s\n",
            "iter 7300, loss 0.8608, sim_pos -0.9392, sim_neg -1.3964, sim_diff 0.4572, time 16.41s\n",
            "iter 7400, loss 0.5075, sim_pos -0.5639, sim_neg -1.5755, sim_diff 1.0117, time 16.26s\n",
            "iter 7500, loss 0.8054, sim_pos -0.7613, sim_neg -1.1882, sim_diff 0.4269, time 16.38s\n",
            "iter 7600, loss 0.7522, sim_pos -0.8217, sim_neg -1.3609, sim_diff 0.5392, time 16.52s\n",
            "iter 7700, loss 0.7864, sim_pos -1.0282, sim_neg -1.6670, sim_diff 0.6388, time 16.61s\n",
            "iter 7800, loss 0.9048, sim_pos -0.9435, sim_neg -1.2028, sim_diff 0.2592, time 16.39s\n",
            "iter 7900, loss 0.6160, sim_pos -0.9218, sim_neg -1.9800, sim_diff 1.0582, time 16.43s\n",
            "iter 8000, loss 0.6942, sim_pos -0.7872, sim_neg -1.6049, sim_diff 0.8177, val/pair_auc 0.7724, val/triplet_acc 0.7900, time 23.62s\n",
            "iter 8100, loss 0.7263, sim_pos -0.9281, sim_neg -1.9455, sim_diff 1.0174, time 16.54s\n",
            "iter 8200, loss 0.7314, sim_pos -0.8001, sim_neg -1.5986, sim_diff 0.7985, time 16.34s\n",
            "iter 8300, loss 0.6245, sim_pos -0.8031, sim_neg -1.6908, sim_diff 0.8877, time 16.56s\n",
            "iter 8400, loss 0.4895, sim_pos -0.5511, sim_neg -1.7146, sim_diff 1.1634, time 16.56s\n",
            "iter 8500, loss 0.7874, sim_pos -0.8201, sim_neg -1.3463, sim_diff 0.5262, time 16.51s\n",
            "iter 8600, loss 0.7034, sim_pos -0.6918, sim_neg -1.3845, sim_diff 0.6926, time 16.66s\n",
            "iter 8700, loss 0.6150, sim_pos -0.7546, sim_neg -1.8653, sim_diff 1.1106, time 16.40s\n",
            "iter 8800, loss 0.7372, sim_pos -0.8766, sim_neg -1.4721, sim_diff 0.5955, time 16.28s\n",
            "iter 8900, loss 0.7060, sim_pos -0.7309, sim_neg -1.4882, sim_diff 0.7573, time 16.35s\n",
            "iter 9000, loss 0.7496, sim_pos -1.0377, sim_neg -1.6844, sim_diff 0.6467, val/pair_auc 0.8000, val/triplet_acc 0.7950, time 23.45s\n",
            "iter 9100, loss 0.8421, sim_pos -1.0695, sim_neg -1.5088, sim_diff 0.4393, time 16.54s\n",
            "iter 9200, loss 0.6331, sim_pos -0.6830, sim_neg -1.5357, sim_diff 0.8528, time 16.42s\n",
            "iter 9300, loss 0.5640, sim_pos -0.6119, sim_neg -1.6215, sim_diff 1.0096, time 16.42s\n",
            "iter 9400, loss 0.7166, sim_pos -0.8787, sim_neg -1.8158, sim_diff 0.9371, time 16.39s\n",
            "iter 9500, loss 0.9679, sim_pos -1.0795, sim_neg -1.2876, sim_diff 0.2081, time 16.65s\n",
            "iter 9600, loss 0.7288, sim_pos -0.7881, sim_neg -1.5754, sim_diff 0.7873, time 16.53s\n",
            "iter 9700, loss 0.8098, sim_pos -1.1634, sim_neg -1.7475, sim_diff 0.5841, time 16.45s\n",
            "iter 9800, loss 0.8032, sim_pos -1.0656, sim_neg -1.7707, sim_diff 0.7051, time 16.61s\n",
            "iter 9900, loss 0.7105, sim_pos -1.0725, sim_neg -2.1292, sim_diff 1.0567, time 16.43s\n",
            "iter 10000, loss 0.9144, sim_pos -0.8796, sim_neg -1.0608, sim_diff 0.1813, val/pair_auc 0.7836, val/triplet_acc 0.8140, time 23.59s\n",
            "iter 10100, loss 0.6147, sim_pos -0.8220, sim_neg -1.9079, sim_diff 1.0859, time 16.40s\n",
            "iter 10200, loss 0.8417, sim_pos -0.8031, sim_neg -1.1549, sim_diff 0.3518, time 16.36s\n",
            "iter 10300, loss 0.7558, sim_pos -0.9889, sim_neg -1.6811, sim_diff 0.6922, time 16.58s\n",
            "iter 10400, loss 0.6295, sim_pos -0.9587, sim_neg -2.1891, sim_diff 1.2304, time 16.27s\n",
            "iter 10500, loss 0.7142, sim_pos -0.8076, sim_neg -1.4734, sim_diff 0.6659, time 16.35s\n",
            "iter 10600, loss 0.7432, sim_pos -0.9018, sim_neg -1.5177, sim_diff 0.6159, time 16.27s\n",
            "iter 10700, loss 0.8652, sim_pos -1.1643, sim_neg -1.5391, sim_diff 0.3748, time 16.24s\n",
            "iter 10800, loss 0.6783, sim_pos -0.7714, sim_neg -1.4912, sim_diff 0.7198, time 16.26s\n",
            "iter 10900, loss 0.7613, sim_pos -1.0974, sim_neg -1.9048, sim_diff 0.8074, time 16.56s\n",
            "iter 11000, loss 0.6287, sim_pos -0.9141, sim_neg -2.2957, sim_diff 1.3816, val/pair_auc 0.7836, val/triplet_acc 0.8080, time 23.28s\n",
            "iter 11100, loss 0.7252, sim_pos -0.5495, sim_neg -1.3571, sim_diff 0.8076, time 16.29s\n",
            "iter 11200, loss 0.6730, sim_pos -0.9480, sim_neg -1.6593, sim_diff 0.7113, time 16.20s\n",
            "iter 11300, loss 0.7752, sim_pos -0.8639, sim_neg -1.3472, sim_diff 0.4833, time 16.48s\n",
            "iter 11400, loss 0.7867, sim_pos -0.9051, sim_neg -1.3888, sim_diff 0.4837, time 16.26s\n",
            "iter 11500, loss 0.7159, sim_pos -0.9124, sim_neg -1.8646, sim_diff 0.9521, time 16.27s\n",
            "iter 11600, loss 0.6806, sim_pos -0.8755, sim_neg -1.9367, sim_diff 1.0612, time 16.25s\n",
            "iter 11700, loss 0.6395, sim_pos -0.8144, sim_neg -1.8952, sim_diff 1.0808, time 16.21s\n",
            "iter 11800, loss 0.5509, sim_pos -0.8375, sim_neg -2.1201, sim_diff 1.2826, time 16.23s\n",
            "iter 11900, loss 0.7295, sim_pos -0.9818, sim_neg -1.7891, sim_diff 0.8073, time 16.38s\n",
            "iter 12000, loss 0.5681, sim_pos -0.6242, sim_neg -1.8433, sim_diff 1.2190, val/pair_auc 0.7918, val/triplet_acc 0.8030, time 23.07s\n",
            "iter 12100, loss 0.7130, sim_pos -0.7903, sim_neg -1.3698, sim_diff 0.5795, time 16.27s\n",
            "iter 12200, loss 0.5716, sim_pos -0.6673, sim_neg -1.8757, sim_diff 1.2084, time 16.36s\n",
            "iter 12300, loss 0.7782, sim_pos -0.8168, sim_neg -1.3926, sim_diff 0.5758, time 16.47s\n",
            "iter 12400, loss 0.6779, sim_pos -0.8629, sim_neg -1.7244, sim_diff 0.8615, time 16.36s\n",
            "iter 12500, loss 0.7201, sim_pos -0.7418, sim_neg -1.4595, sim_diff 0.7177, time 16.09s\n",
            "iter 12600, loss 0.7427, sim_pos -0.8005, sim_neg -1.3151, sim_diff 0.5146, time 16.38s\n",
            "iter 12700, loss 0.7307, sim_pos -1.1258, sim_neg -2.0698, sim_diff 0.9441, time 16.26s\n",
            "iter 12800, loss 0.8090, sim_pos -1.0602, sim_neg -1.4867, sim_diff 0.4265, time 16.41s\n",
            "iter 12900, loss 0.9477, sim_pos -1.0340, sim_neg -1.2968, sim_diff 0.2628, time 16.16s\n",
            "iter 13000, loss 0.6899, sim_pos -0.8394, sim_neg -1.5410, sim_diff 0.7016, val/pair_auc 0.7982, val/triplet_acc 0.8060, time 23.42s\n",
            "iter 13100, loss 0.4866, sim_pos -0.7310, sim_neg -2.0833, sim_diff 1.3522, time 16.14s\n",
            "iter 13200, loss 0.5393, sim_pos -0.7013, sim_neg -2.0840, sim_diff 1.3827, time 16.48s\n",
            "iter 13300, loss 0.7887, sim_pos -0.9290, sim_neg -1.5640, sim_diff 0.6349, time 16.38s\n",
            "iter 13400, loss 0.7169, sim_pos -1.0227, sim_neg -1.8168, sim_diff 0.7941, time 16.27s\n",
            "iter 13500, loss 0.6338, sim_pos -0.7336, sim_neg -1.6033, sim_diff 0.8697, time 16.37s\n",
            "iter 13600, loss 0.7644, sim_pos -1.1719, sim_neg -1.9862, sim_diff 0.8143, time 16.32s\n",
            "iter 13700, loss 0.7076, sim_pos -0.8307, sim_neg -1.6929, sim_diff 0.8622, time 16.49s\n",
            "iter 13800, loss 0.8072, sim_pos -0.8947, sim_neg -1.2803, sim_diff 0.3857, time 16.19s\n",
            "iter 13900, loss 0.7728, sim_pos -1.0009, sim_neg -1.6822, sim_diff 0.6813, time 16.28s\n",
            "iter 14000, loss 0.7584, sim_pos -1.0916, sim_neg -1.7041, sim_diff 0.6126, val/pair_auc 0.7960, val/triplet_acc 0.8180, time 23.39s\n",
            "iter 14100, loss 0.5559, sim_pos -0.7873, sim_neg -1.8943, sim_diff 1.1070, time 16.46s\n",
            "iter 14200, loss 0.9140, sim_pos -0.8041, sim_neg -1.0101, sim_diff 0.2060, time 16.67s\n",
            "iter 14300, loss 0.6194, sim_pos -1.0453, sim_neg -1.9993, sim_diff 0.9540, time 16.46s\n",
            "iter 14400, loss 0.6944, sim_pos -0.9581, sim_neg -1.7452, sim_diff 0.7871, time 16.33s\n",
            "iter 14500, loss 0.6382, sim_pos -0.9115, sim_neg -1.8722, sim_diff 0.9608, time 16.29s\n",
            "iter 14600, loss 0.8153, sim_pos -0.9963, sim_neg -1.3863, sim_diff 0.3900, time 16.55s\n",
            "iter 14700, loss 0.6512, sim_pos -0.9143, sim_neg -1.7673, sim_diff 0.8529, time 16.35s\n",
            "iter 14800, loss 0.8950, sim_pos -1.2229, sim_neg -1.4866, sim_diff 0.2637, time 16.41s\n",
            "iter 14900, loss 0.6700, sim_pos -0.6920, sim_neg -1.4534, sim_diff 0.7614, time 16.24s\n",
            "iter 15000, loss 0.7847, sim_pos -0.9382, sim_neg -1.3785, sim_diff 0.4403, val/pair_auc 0.8002, val/triplet_acc 0.8150, time 23.65s\n",
            "iter 15100, loss 0.5855, sim_pos -0.7736, sim_neg -1.7464, sim_diff 0.9728, time 16.21s\n",
            "iter 15200, loss 0.7767, sim_pos -0.9225, sim_neg -1.4226, sim_diff 0.5001, time 16.33s\n",
            "iter 15300, loss 0.7170, sim_pos -1.0253, sim_neg -1.7788, sim_diff 0.7535, time 16.47s\n",
            "iter 15400, loss 0.6277, sim_pos -0.8530, sim_neg -1.7451, sim_diff 0.8921, time 16.62s\n",
            "iter 15500, loss 0.7328, sim_pos -0.8714, sim_neg -1.5818, sim_diff 0.7104, time 16.38s\n",
            "iter 15600, loss 0.6709, sim_pos -0.9636, sim_neg -1.8775, sim_diff 0.9139, time 16.35s\n",
            "iter 15700, loss 0.5866, sim_pos -0.6315, sim_neg -1.5414, sim_diff 0.9099, time 16.43s\n",
            "iter 15800, loss 0.7843, sim_pos -0.8480, sim_neg -1.3853, sim_diff 0.5373, time 16.39s\n",
            "iter 15900, loss 0.6926, sim_pos -0.9985, sim_neg -1.7423, sim_diff 0.7439, time 16.51s\n",
            "iter 16000, loss 0.7955, sim_pos -0.8738, sim_neg -1.3888, sim_diff 0.5149, val/pair_auc 0.8042, val/triplet_acc 0.8140, time 23.36s\n",
            "iter 16100, loss 0.5770, sim_pos -0.6848, sim_neg -1.7370, sim_diff 1.0522, time 16.32s\n",
            "iter 16200, loss 0.7391, sim_pos -0.9921, sim_neg -1.5673, sim_diff 0.5752, time 16.34s\n",
            "iter 16300, loss 0.6864, sim_pos -1.1113, sim_neg -2.3261, sim_diff 1.2148, time 16.65s\n",
            "iter 16400, loss 0.5006, sim_pos -0.7051, sim_neg -1.8741, sim_diff 1.1690, time 16.36s\n",
            "iter 16500, loss 0.8554, sim_pos -0.9645, sim_neg -1.3217, sim_diff 0.3572, time 16.29s\n",
            "iter 16600, loss 0.6413, sim_pos -0.7727, sim_neg -1.5797, sim_diff 0.8071, time 16.39s\n",
            "iter 16700, loss 0.6426, sim_pos -0.9670, sim_neg -2.0767, sim_diff 1.1097, time 16.33s\n",
            "iter 16800, loss 0.8627, sim_pos -0.9994, sim_neg -1.5007, sim_diff 0.5013, time 16.65s\n",
            "iter 16900, loss 0.8721, sim_pos -1.2546, sim_neg -1.7700, sim_diff 0.5154, time 16.33s\n",
            "iter 17000, loss 0.7006, sim_pos -0.8844, sim_neg -1.7083, sim_diff 0.8239, val/pair_auc 0.7978, val/triplet_acc 0.7970, time 23.39s\n",
            "iter 17100, loss 0.7294, sim_pos -0.8695, sim_neg -1.5233, sim_diff 0.6538, time 16.25s\n",
            "iter 17200, loss 0.8105, sim_pos -0.9159, sim_neg -1.3275, sim_diff 0.4117, time 16.55s\n",
            "iter 17300, loss 1.0011, sim_pos -1.0854, sim_neg -1.0832, sim_diff -0.0022, time 16.35s\n",
            "iter 17400, loss 0.7979, sim_pos -0.9491, sim_neg -1.3936, sim_diff 0.4445, time 16.38s\n",
            "iter 17500, loss 0.7470, sim_pos -0.9961, sim_neg -1.8211, sim_diff 0.8250, time 16.40s\n",
            "iter 17600, loss 0.7754, sim_pos -1.0004, sim_neg -1.6437, sim_diff 0.6433, time 16.54s\n",
            "iter 17700, loss 0.6581, sim_pos -0.8462, sim_neg -1.6331, sim_diff 0.7869, time 16.55s\n",
            "iter 17800, loss 0.5499, sim_pos -0.7553, sim_neg -1.7794, sim_diff 1.0241, time 16.37s\n",
            "iter 17900, loss 0.7610, sim_pos -0.8504, sim_neg -1.5260, sim_diff 0.6756, time 16.30s\n",
            "iter 18000, loss 0.8393, sim_pos -1.0508, sim_neg -1.4001, sim_diff 0.3493, val/pair_auc 0.8156, val/triplet_acc 0.8130, time 23.31s\n",
            "iter 18100, loss 0.6833, sim_pos -0.8772, sim_neg -1.9229, sim_diff 1.0457, time 16.29s\n",
            "iter 18200, loss 0.6313, sim_pos -0.8805, sim_neg -1.8744, sim_diff 0.9939, time 16.54s\n",
            "iter 18300, loss 0.4550, sim_pos -0.6461, sim_neg -2.0080, sim_diff 1.3619, time 16.44s\n",
            "iter 18400, loss 0.7194, sim_pos -1.0005, sim_neg -1.6556, sim_diff 0.6551, time 16.35s\n",
            "iter 18500, loss 0.7206, sim_pos -1.0230, sim_neg -1.6853, sim_diff 0.6623, time 16.39s\n",
            "iter 18600, loss 0.8121, sim_pos -0.8240, sim_neg -1.2808, sim_diff 0.4568, time 16.53s\n",
            "iter 18700, loss 0.7067, sim_pos -0.8806, sim_neg -1.6781, sim_diff 0.7975, time 16.37s\n",
            "iter 18800, loss 0.6891, sim_pos -0.9699, sim_neg -1.6517, sim_diff 0.6818, time 16.37s\n",
            "iter 18900, loss 0.6652, sim_pos -0.8758, sim_neg -1.5650, sim_diff 0.6892, time 16.38s\n",
            "iter 19000, loss 0.6708, sim_pos -0.7916, sim_neg -1.5049, sim_diff 0.7133, val/pair_auc 0.7986, val/triplet_acc 0.8150, time 23.39s\n",
            "iter 19100, loss 0.7562, sim_pos -1.0327, sim_neg -2.0130, sim_diff 0.9802, time 16.54s\n",
            "iter 19200, loss 0.7932, sim_pos -1.0789, sim_neg -1.6783, sim_diff 0.5994, time 16.34s\n",
            "iter 19300, loss 0.5048, sim_pos -0.7735, sim_neg -2.0126, sim_diff 1.2391, time 16.25s\n",
            "iter 19400, loss 0.6203, sim_pos -0.8742, sim_neg -1.7076, sim_diff 0.8334, time 16.29s\n",
            "iter 19500, loss 0.6233, sim_pos -0.8642, sim_neg -1.7647, sim_diff 0.9005, time 16.29s\n",
            "iter 19600, loss 0.6646, sim_pos -0.7714, sim_neg -1.5240, sim_diff 0.7526, time 16.52s\n",
            "iter 19700, loss 0.4916, sim_pos -0.8570, sim_neg -2.3203, sim_diff 1.4633, time 16.30s\n",
            "iter 19800, loss 0.7348, sim_pos -0.7283, sim_neg -1.3926, sim_diff 0.6642, time 16.33s\n",
            "iter 19900, loss 0.8326, sim_pos -1.0646, sim_neg -1.5557, sim_diff 0.4910, time 16.48s\n",
            "iter 20000, loss 0.6656, sim_pos -0.8212, sim_neg -1.6955, sim_diff 0.8743, val/pair_auc 0.8254, val/triplet_acc 0.8280, time 23.39s\n",
            "iter 20100, loss 0.5978, sim_pos -0.8248, sim_neg -1.8923, sim_diff 1.0675, time 16.63s\n",
            "iter 20200, loss 0.6201, sim_pos -0.9729, sim_neg -2.0576, sim_diff 1.0847, time 16.36s\n",
            "iter 20300, loss 0.6755, sim_pos -0.8526, sim_neg -1.6743, sim_diff 0.8217, time 16.37s\n",
            "iter 20400, loss 0.6680, sim_pos -0.9636, sim_neg -1.8174, sim_diff 0.8538, time 16.39s\n",
            "iter 20500, loss 0.6228, sim_pos -0.9404, sim_neg -1.8185, sim_diff 0.8781, time 16.58s\n",
            "iter 20600, loss 0.6581, sim_pos -0.9008, sim_neg -1.9534, sim_diff 1.0527, time 16.34s\n",
            "iter 20700, loss 0.5929, sim_pos -0.7853, sim_neg -1.7163, sim_diff 0.9311, time 16.42s\n",
            "iter 20800, loss 0.7013, sim_pos -0.8378, sim_neg -1.7799, sim_diff 0.9421, time 16.35s\n",
            "iter 20900, loss 0.6774, sim_pos -0.8985, sim_neg -1.6625, sim_diff 0.7640, time 16.44s\n",
            "iter 21000, loss 0.5508, sim_pos -0.7687, sim_neg -1.8721, sim_diff 1.1034, val/pair_auc 0.8076, val/triplet_acc 0.8140, time 23.72s\n",
            "iter 21100, loss 0.8776, sim_pos -1.2173, sim_neg -1.6309, sim_diff 0.4137, time 16.26s\n",
            "iter 21200, loss 0.5428, sim_pos -0.8630, sim_neg -2.3020, sim_diff 1.4390, time 16.42s\n",
            "iter 21300, loss 0.6083, sim_pos -0.8062, sim_neg -1.7081, sim_diff 0.9019, time 16.40s\n",
            "iter 21400, loss 0.6757, sim_pos -1.0309, sim_neg -1.8905, sim_diff 0.8595, time 16.58s\n",
            "iter 21500, loss 0.8091, sim_pos -0.8857, sim_neg -1.2675, sim_diff 0.3818, time 16.29s\n",
            "iter 21600, loss 0.7819, sim_pos -0.8950, sim_neg -1.3851, sim_diff 0.4901, time 16.40s\n",
            "iter 21700, loss 0.7469, sim_pos -0.9041, sim_neg -1.6216, sim_diff 0.7175, time 16.52s\n",
            "iter 21800, loss 0.6107, sim_pos -0.8314, sim_neg -1.9790, sim_diff 1.1476, time 16.63s\n",
            "iter 21900, loss 0.7176, sim_pos -1.0147, sim_neg -1.6326, sim_diff 0.6179, time 16.37s\n",
            "iter 22000, loss 0.6744, sim_pos -0.9421, sim_neg -1.9670, sim_diff 1.0249, val/pair_auc 0.8190, val/triplet_acc 0.8230, time 23.46s\n",
            "iter 22100, loss 0.6997, sim_pos -1.0141, sim_neg -1.9935, sim_diff 0.9794, time 16.32s\n",
            "iter 22200, loss 0.6270, sim_pos -0.9024, sim_neg -1.8523, sim_diff 0.9499, time 16.28s\n",
            "iter 22300, loss 0.7193, sim_pos -0.8821, sim_neg -1.6380, sim_diff 0.7559, time 16.51s\n",
            "iter 22400, loss 0.6775, sim_pos -0.9451, sim_neg -1.9360, sim_diff 0.9909, time 16.45s\n",
            "iter 22500, loss 0.5871, sim_pos -0.7429, sim_neg -1.5828, sim_diff 0.8400, time 16.44s\n",
            "iter 22600, loss 0.6069, sim_pos -0.8166, sim_neg -1.6199, sim_diff 0.8034, time 16.48s\n",
            "iter 22700, loss 0.7105, sim_pos -0.9431, sim_neg -1.8867, sim_diff 0.9436, time 16.36s\n",
            "iter 22800, loss 0.5730, sim_pos -0.7049, sim_neg -1.7093, sim_diff 1.0044, time 16.53s\n",
            "iter 22900, loss 0.6225, sim_pos -0.9557, sim_neg -1.9321, sim_diff 0.9764, time 16.44s\n",
            "iter 23000, loss 0.6722, sim_pos -0.9091, sim_neg -1.6997, sim_diff 0.7907, val/pair_auc 0.8204, val/triplet_acc 0.8260, time 23.48s\n",
            "iter 23100, loss 0.7528, sim_pos -0.9303, sim_neg -1.6836, sim_diff 0.7533, time 16.39s\n",
            "iter 23200, loss 0.6775, sim_pos -1.1191, sim_neg -2.0071, sim_diff 0.8880, time 16.47s\n",
            "iter 23300, loss 0.7468, sim_pos -1.0886, sim_neg -1.7977, sim_diff 0.7090, time 16.59s\n",
            "iter 23400, loss 0.4713, sim_pos -0.7123, sim_neg -2.0066, sim_diff 1.2942, time 16.36s\n",
            "iter 23500, loss 0.7404, sim_pos -1.0304, sim_neg -1.6906, sim_diff 0.6602, time 16.44s\n",
            "iter 23600, loss 0.7806, sim_pos -1.0843, sim_neg -1.6437, sim_diff 0.5594, time 16.49s\n",
            "iter 23700, loss 0.6488, sim_pos -0.7553, sim_neg -1.7664, sim_diff 1.0111, time 16.61s\n",
            "iter 23800, loss 0.7047, sim_pos -0.8653, sim_neg -1.6060, sim_diff 0.7407, time 16.54s\n",
            "iter 23900, loss 0.5733, sim_pos -0.8123, sim_neg -1.7698, sim_diff 0.9575, time 16.34s\n",
            "iter 24000, loss 0.6969, sim_pos -0.9363, sim_neg -1.7043, sim_diff 0.7679, val/pair_auc 0.8298, val/triplet_acc 0.8400, time 23.38s\n",
            "iter 24100, loss 0.6439, sim_pos -0.9016, sim_neg -1.7788, sim_diff 0.8772, time 16.52s\n",
            "iter 24200, loss 0.6799, sim_pos -0.9387, sim_neg -1.6136, sim_diff 0.6749, time 16.47s\n",
            "iter 24300, loss 0.6390, sim_pos -0.8359, sim_neg -1.6232, sim_diff 0.7872, time 16.39s\n",
            "iter 24400, loss 0.7832, sim_pos -0.7688, sim_neg -1.2024, sim_diff 0.4336, time 16.42s\n",
            "iter 24500, loss 0.5085, sim_pos -0.7809, sim_neg -2.0267, sim_diff 1.2458, time 16.40s\n",
            "iter 24600, loss 0.8129, sim_pos -0.9531, sim_neg -1.4684, sim_diff 0.5153, time 16.56s\n",
            "iter 24700, loss 0.6607, sim_pos -0.6466, sim_neg -1.3252, sim_diff 0.6786, time 16.30s\n",
            "iter 24800, loss 0.7174, sim_pos -0.8953, sim_neg -1.5741, sim_diff 0.6788, time 16.44s\n",
            "iter 24900, loss 0.6468, sim_pos -0.7951, sim_neg -1.8453, sim_diff 1.0502, time 16.36s\n",
            "iter 25000, loss 0.5683, sim_pos -0.8286, sim_neg -2.3547, sim_diff 1.5261, val/pair_auc 0.8254, val/triplet_acc 0.8360, time 23.31s\n",
            "iter 25100, loss 0.6220, sim_pos -0.6743, sim_neg -1.6688, sim_diff 0.9944, time 16.60s\n",
            "iter 25200, loss 0.7055, sim_pos -0.9859, sim_neg -1.6621, sim_diff 0.6762, time 16.39s\n",
            "iter 25300, loss 0.7025, sim_pos -0.9371, sim_neg -1.5432, sim_diff 0.6061, time 16.34s\n",
            "iter 25400, loss 0.7141, sim_pos -0.8600, sim_neg -1.5610, sim_diff 0.7010, time 16.37s\n",
            "iter 25500, loss 0.6834, sim_pos -0.9726, sim_neg -1.8283, sim_diff 0.8558, time 16.45s\n",
            "iter 25600, loss 0.7039, sim_pos -1.0846, sim_neg -1.8029, sim_diff 0.7183, time 16.58s\n",
            "iter 25700, loss 0.7986, sim_pos -0.9663, sim_neg -1.4803, sim_diff 0.5140, time 16.44s\n",
            "iter 25800, loss 0.5654, sim_pos -0.7700, sim_neg -1.9305, sim_diff 1.1604, time 16.37s\n",
            "iter 25900, loss 0.8205, sim_pos -0.8433, sim_neg -1.2022, sim_diff 0.3589, time 16.51s\n",
            "iter 26000, loss 0.5498, sim_pos -0.7546, sim_neg -1.7568, sim_diff 1.0022, val/pair_auc 0.8468, val/triplet_acc 0.8440, time 23.28s\n",
            "iter 26100, loss 0.6141, sim_pos -0.8330, sim_neg -1.7719, sim_diff 0.9389, time 16.48s\n",
            "iter 26200, loss 0.7096, sim_pos -0.9924, sim_neg -1.8392, sim_diff 0.8468, time 16.30s\n",
            "iter 26300, loss 0.7093, sim_pos -1.0440, sim_neg -1.9239, sim_diff 0.8799, time 16.20s\n",
            "iter 26400, loss 0.5506, sim_pos -0.7245, sim_neg -1.7534, sim_diff 1.0288, time 16.26s\n",
            "iter 26500, loss 0.7274, sim_pos -0.9651, sim_neg -1.6058, sim_diff 0.6406, time 16.22s\n",
            "iter 26600, loss 0.6836, sim_pos -0.8797, sim_neg -1.8484, sim_diff 0.9687, time 16.45s\n",
            "iter 26700, loss 0.7637, sim_pos -1.0233, sim_neg -1.5373, sim_diff 0.5140, time 16.49s\n",
            "iter 26800, loss 0.6890, sim_pos -1.0921, sim_neg -1.8592, sim_diff 0.7671, time 16.33s\n",
            "iter 26900, loss 0.6555, sim_pos -0.9096, sim_neg -1.6947, sim_diff 0.7851, time 16.28s\n",
            "iter 27000, loss 0.7983, sim_pos -0.8457, sim_neg -1.3124, sim_diff 0.4667, val/pair_auc 0.8276, val/triplet_acc 0.8310, time 23.33s\n",
            "iter 27100, loss 0.7503, sim_pos -1.3453, sim_neg -2.2048, sim_diff 0.8594, time 16.47s\n",
            "iter 27200, loss 0.7535, sim_pos -1.0623, sim_neg -1.7008, sim_diff 0.6385, time 16.38s\n",
            "iter 27300, loss 0.7169, sim_pos -1.0073, sim_neg -1.5856, sim_diff 0.5783, time 16.36s\n",
            "iter 27400, loss 0.8231, sim_pos -1.0073, sim_neg -1.6100, sim_diff 0.6027, time 16.31s\n",
            "iter 27500, loss 0.6712, sim_pos -0.9823, sim_neg -2.0374, sim_diff 1.0551, time 16.41s\n",
            "iter 27600, loss 0.6503, sim_pos -0.8792, sim_neg -1.7585, sim_diff 0.8793, time 16.37s\n",
            "iter 27700, loss 0.6793, sim_pos -1.0304, sim_neg -1.8645, sim_diff 0.8341, time 16.57s\n",
            "iter 27800, loss 0.8075, sim_pos -1.1174, sim_neg -1.6656, sim_diff 0.5482, time 16.35s\n",
            "iter 27900, loss 0.6993, sim_pos -0.8439, sim_neg -1.5954, sim_diff 0.7515, time 16.47s\n",
            "iter 28000, loss 0.6471, sim_pos -0.9382, sim_neg -1.7353, sim_diff 0.7971, val/pair_auc 0.8324, val/triplet_acc 0.8210, time 23.35s\n",
            "iter 28100, loss 0.6216, sim_pos -0.9282, sim_neg -2.0225, sim_diff 1.0943, time 16.49s\n",
            "iter 28200, loss 0.5924, sim_pos -0.8570, sim_neg -1.8438, sim_diff 0.9867, time 16.34s\n",
            "iter 28300, loss 0.7545, sim_pos -0.9495, sim_neg -1.5247, sim_diff 0.5752, time 16.45s\n",
            "iter 28400, loss 0.6312, sim_pos -0.8549, sim_neg -1.9003, sim_diff 1.0453, time 16.51s\n",
            "iter 28500, loss 0.5373, sim_pos -0.6384, sim_neg -1.8152, sim_diff 1.1767, time 16.39s\n",
            "iter 28600, loss 0.8159, sim_pos -0.8144, sim_neg -1.1827, sim_diff 0.3682, time 16.52s\n",
            "iter 28700, loss 0.7690, sim_pos -0.9164, sim_neg -1.4059, sim_diff 0.4895, time 16.33s\n",
            "iter 28800, loss 0.7714, sim_pos -0.8442, sim_neg -1.3285, sim_diff 0.4843, time 16.34s\n",
            "iter 28900, loss 0.7067, sim_pos -1.0882, sim_neg -1.9403, sim_diff 0.8521, time 16.66s\n",
            "iter 29000, loss 0.7766, sim_pos -0.9879, sim_neg -1.9757, sim_diff 0.9878, val/pair_auc 0.8294, val/triplet_acc 0.8430, time 23.65s\n",
            "iter 29100, loss 0.7942, sim_pos -0.8738, sim_neg -1.3755, sim_diff 0.5018, time 16.42s\n",
            "iter 29200, loss 0.6352, sim_pos -0.8014, sim_neg -1.6027, sim_diff 0.8013, time 16.48s\n",
            "iter 29300, loss 0.6351, sim_pos -0.7966, sim_neg -1.6256, sim_diff 0.8290, time 16.60s\n",
            "iter 29400, loss 0.6181, sim_pos -0.7928, sim_neg -1.7018, sim_diff 0.9090, time 16.40s\n",
            "iter 29500, loss 0.8562, sim_pos -0.9625, sim_neg -1.2502, sim_diff 0.2877, time 16.33s\n",
            "iter 29600, loss 0.6478, sim_pos -0.9962, sim_neg -2.1476, sim_diff 1.1514, time 16.36s\n",
            "iter 29700, loss 0.6374, sim_pos -0.7273, sim_neg -1.5154, sim_diff 0.7881, time 16.48s\n",
            "iter 29800, loss 0.6757, sim_pos -0.8309, sim_neg -1.5451, sim_diff 0.7143, time 16.67s\n",
            "iter 29900, loss 0.7579, sim_pos -1.1876, sim_neg -1.9655, sim_diff 0.7779, time 16.38s\n",
            "iter 30000, loss 0.4941, sim_pos -0.6990, sim_neg -2.1143, sim_diff 1.4153, val/pair_auc 0.8360, val/triplet_acc 0.8310, time 23.21s\n",
            "iter 30100, loss 0.6919, sim_pos -0.8010, sim_neg -1.4923, sim_diff 0.6914, time 16.64s\n",
            "iter 30200, loss 0.7167, sim_pos -0.9651, sim_neg -1.5642, sim_diff 0.5990, time 16.75s\n",
            "iter 30300, loss 0.6560, sim_pos -0.7823, sim_neg -1.5877, sim_diff 0.8054, time 16.72s\n",
            "iter 30400, loss 0.7212, sim_pos -0.9230, sim_neg -1.6843, sim_diff 0.7613, time 16.21s\n",
            "iter 30500, loss 0.7430, sim_pos -1.0344, sim_neg -1.7356, sim_diff 0.7011, time 16.29s\n",
            "iter 30600, loss 0.5928, sim_pos -0.8890, sim_neg -1.8911, sim_diff 1.0021, time 16.49s\n",
            "iter 30700, loss 0.8479, sim_pos -1.1255, sim_neg -1.4346, sim_diff 0.3091, time 16.32s\n",
            "iter 30800, loss 0.8140, sim_pos -1.0232, sim_neg -1.6474, sim_diff 0.6242, time 16.33s\n",
            "iter 30900, loss 0.6199, sim_pos -0.8224, sim_neg -1.8337, sim_diff 1.0113, time 16.27s\n",
            "iter 31000, loss 0.7685, sim_pos -1.0091, sim_neg -1.5168, sim_diff 0.5077, val/pair_auc 0.8472, val/triplet_acc 0.8490, time 23.17s\n",
            "iter 31100, loss 0.8642, sim_pos -1.0869, sim_neg -1.4024, sim_diff 0.3155, time 16.42s\n",
            "iter 31200, loss 0.7772, sim_pos -1.1344, sim_neg -1.7981, sim_diff 0.6637, time 16.26s\n",
            "iter 31300, loss 0.5934, sim_pos -0.7475, sim_neg -1.7937, sim_diff 1.0461, time 16.31s\n",
            "iter 31400, loss 0.5688, sim_pos -0.7977, sim_neg -2.1271, sim_diff 1.3293, time 16.29s\n",
            "iter 31500, loss 0.7297, sim_pos -1.1396, sim_neg -2.0017, sim_diff 0.8620, time 16.46s\n",
            "iter 31600, loss 0.6068, sim_pos -0.9435, sim_neg -2.0547, sim_diff 1.1112, time 16.29s\n",
            "iter 31700, loss 0.8213, sim_pos -1.2040, sim_neg -1.5613, sim_diff 0.3574, time 16.32s\n",
            "iter 31800, loss 0.8183, sim_pos -1.0685, sim_neg -1.5333, sim_diff 0.4648, time 16.24s\n",
            "iter 31900, loss 0.4707, sim_pos -0.5933, sim_neg -1.8010, sim_diff 1.2078, time 16.23s\n",
            "iter 32000, loss 0.7971, sim_pos -1.1639, sim_neg -1.6966, sim_diff 0.5328, val/pair_auc 0.8368, val/triplet_acc 0.8500, time 23.35s\n",
            "iter 32100, loss 0.7220, sim_pos -0.9327, sim_neg -1.9875, sim_diff 1.0549, time 16.12s\n",
            "iter 32200, loss 0.7989, sim_pos -1.0894, sim_neg -1.6570, sim_diff 0.5676, time 16.30s\n",
            "iter 32300, loss 0.5951, sim_pos -0.8580, sim_neg -1.9234, sim_diff 1.0654, time 16.11s\n",
            "iter 32400, loss 0.8705, sim_pos -1.2601, sim_neg -1.5945, sim_diff 0.3344, time 16.24s\n",
            "iter 32500, loss 0.5933, sim_pos -0.8394, sim_neg -2.1020, sim_diff 1.2626, time 16.22s\n",
            "iter 32600, loss 0.7778, sim_pos -0.7903, sim_neg -1.3715, sim_diff 0.5813, time 16.45s\n",
            "iter 32700, loss 0.6553, sim_pos -0.9006, sim_neg -1.8339, sim_diff 0.9333, time 16.32s\n",
            "iter 32800, loss 0.7285, sim_pos -1.0091, sim_neg -1.5605, sim_diff 0.5514, time 16.27s\n",
            "iter 32900, loss 0.8058, sim_pos -0.9493, sim_neg -1.3804, sim_diff 0.4312, time 16.24s\n",
            "iter 33000, loss 0.6623, sim_pos -0.6888, sim_neg -1.4666, sim_diff 0.7778, val/pair_auc 0.8284, val/triplet_acc 0.8470, time 23.26s\n",
            "iter 33100, loss 0.6442, sim_pos -0.8496, sim_neg -1.8776, sim_diff 1.0280, time 16.46s\n",
            "iter 33200, loss 0.6953, sim_pos -0.7962, sim_neg -1.4208, sim_diff 0.6246, time 16.25s\n",
            "iter 33300, loss 0.6786, sim_pos -0.8723, sim_neg -1.7662, sim_diff 0.8938, time 16.34s\n",
            "iter 33400, loss 0.8243, sim_pos -1.1127, sim_neg -1.6465, sim_diff 0.5338, time 16.34s\n",
            "iter 33500, loss 0.8136, sim_pos -1.2270, sim_neg -1.7594, sim_diff 0.5324, time 16.28s\n",
            "iter 33600, loss 0.6256, sim_pos -0.7509, sim_neg -1.6375, sim_diff 0.8866, time 16.51s\n",
            "iter 33700, loss 0.7927, sim_pos -0.8573, sim_neg -1.3449, sim_diff 0.4876, time 16.41s\n",
            "iter 33800, loss 0.7025, sim_pos -0.9766, sim_neg -1.6023, sim_diff 0.6257, time 16.32s\n",
            "iter 33900, loss 0.8202, sim_pos -1.1105, sim_neg -1.6201, sim_diff 0.5096, time 16.28s\n",
            "iter 34000, loss 0.6163, sim_pos -1.0408, sim_neg -2.0480, sim_diff 1.0072, val/pair_auc 0.8414, val/triplet_acc 0.8490, time 23.90s\n",
            "iter 34100, loss 0.7023, sim_pos -0.9827, sim_neg -1.8154, sim_diff 0.8327, time 16.11s\n",
            "iter 34200, loss 0.7124, sim_pos -0.8433, sim_neg -1.4752, sim_diff 0.6319, time 16.19s\n",
            "iter 34300, loss 0.7055, sim_pos -0.8340, sim_neg -1.4940, sim_diff 0.6600, time 16.15s\n",
            "iter 34400, loss 0.7287, sim_pos -1.0930, sim_neg -1.6946, sim_diff 0.6017, time 16.33s\n",
            "iter 34500, loss 0.7930, sim_pos -1.1123, sim_neg -1.6597, sim_diff 0.5474, time 16.43s\n",
            "iter 34600, loss 0.7140, sim_pos -1.1900, sim_neg -2.1070, sim_diff 0.9170, time 16.58s\n",
            "iter 34700, loss 0.8111, sim_pos -1.0605, sim_neg -1.4384, sim_diff 0.3779, time 16.70s\n",
            "iter 34800, loss 0.7815, sim_pos -1.2000, sim_neg -2.0627, sim_diff 0.8626, time 16.68s\n",
            "iter 34900, loss 0.5998, sim_pos -0.9275, sim_neg -1.8947, sim_diff 0.9672, time 16.70s\n",
            "iter 35000, loss 0.8144, sim_pos -1.0039, sim_neg -1.3752, sim_diff 0.3713, val/pair_auc 0.8328, val/triplet_acc 0.8330, time 23.72s\n",
            "iter 35100, loss 0.8113, sim_pos -1.0311, sim_neg -1.4658, sim_diff 0.4346, time 16.54s\n",
            "iter 35200, loss 0.6276, sim_pos -0.8279, sim_neg -1.6515, sim_diff 0.8237, time 16.68s\n",
            "iter 35300, loss 0.6622, sim_pos -0.9102, sim_neg -1.7049, sim_diff 0.7947, time 16.85s\n",
            "iter 35400, loss 0.6665, sim_pos -0.9391, sim_neg -1.7114, sim_diff 0.7724, time 16.61s\n",
            "iter 35500, loss 0.6435, sim_pos -0.7461, sim_neg -1.6535, sim_diff 0.9074, time 16.71s\n",
            "iter 35600, loss 0.7207, sim_pos -0.9701, sim_neg -1.6261, sim_diff 0.6560, time 16.71s\n",
            "iter 35700, loss 0.5648, sim_pos -0.9756, sim_neg -2.1469, sim_diff 1.1713, time 16.73s\n",
            "iter 35800, loss 0.6935, sim_pos -0.6840, sim_neg -1.2970, sim_diff 0.6130, time 16.70s\n",
            "iter 35900, loss 0.6268, sim_pos -0.9162, sim_neg -1.9375, sim_diff 1.0213, time 16.87s\n",
            "iter 36000, loss 0.5971, sim_pos -0.7549, sim_neg -1.6792, sim_diff 0.9243, val/pair_auc 0.8302, val/triplet_acc 0.8390, time 23.69s\n",
            "iter 36100, loss 0.6112, sim_pos -0.9147, sim_neg -2.0519, sim_diff 1.1372, time 16.67s\n",
            "iter 36200, loss 0.8730, sim_pos -1.0246, sim_neg -1.2955, sim_diff 0.2709, time 16.80s\n",
            "iter 36300, loss 0.5390, sim_pos -0.8958, sim_neg -2.3079, sim_diff 1.4121, time 16.74s\n",
            "iter 36400, loss 0.6440, sim_pos -0.8556, sim_neg -1.8461, sim_diff 0.9905, time 16.95s\n",
            "iter 36500, loss 0.8345, sim_pos -1.2182, sim_neg -1.8993, sim_diff 0.6811, time 16.69s\n",
            "iter 36600, loss 0.6431, sim_pos -0.8627, sim_neg -1.7078, sim_diff 0.8451, time 16.66s\n",
            "iter 36700, loss 0.8514, sim_pos -1.1874, sim_neg -1.4991, sim_diff 0.3117, time 16.66s\n",
            "iter 36800, loss 0.7998, sim_pos -0.9744, sim_neg -1.3748, sim_diff 0.4005, time 16.91s\n",
            "iter 36900, loss 0.8031, sim_pos -1.0462, sim_neg -1.5304, sim_diff 0.4842, time 16.79s\n",
            "iter 37000, loss 0.5491, sim_pos -0.7917, sim_neg -1.8539, sim_diff 1.0622, val/pair_auc 0.8444, val/triplet_acc 0.8640, time 23.71s\n",
            "iter 37100, loss 0.6014, sim_pos -0.9547, sim_neg -1.9213, sim_diff 0.9666, time 16.57s\n",
            "iter 37200, loss 0.7939, sim_pos -1.0755, sim_neg -1.6382, sim_diff 0.5627, time 16.82s\n",
            "iter 37300, loss 0.6972, sim_pos -0.9870, sim_neg -1.8605, sim_diff 0.8735, time 16.61s\n",
            "iter 37400, loss 0.5828, sim_pos -0.7595, sim_neg -1.6339, sim_diff 0.8744, time 16.59s\n",
            "iter 37500, loss 0.7605, sim_pos -1.2242, sim_neg -1.9344, sim_diff 0.7102, time 16.69s\n",
            "iter 37600, loss 0.4816, sim_pos -0.8405, sim_neg -2.2664, sim_diff 1.4259, time 16.66s\n",
            "iter 37700, loss 0.7488, sim_pos -1.0571, sim_neg -1.6719, sim_diff 0.6149, time 16.86s\n",
            "iter 37800, loss 0.7981, sim_pos -1.1728, sim_neg -1.7596, sim_diff 0.5868, time 16.65s\n",
            "iter 37900, loss 0.7719, sim_pos -0.8061, sim_neg -1.2768, sim_diff 0.4707, time 16.64s\n",
            "iter 38000, loss 0.6243, sim_pos -0.8752, sim_neg -1.7905, sim_diff 0.9154, val/pair_auc 0.8416, val/triplet_acc 0.8560, time 23.68s\n",
            "iter 38100, loss 0.7115, sim_pos -1.1502, sim_neg -2.0478, sim_diff 0.8976, time 16.88s\n",
            "iter 38200, loss 0.7046, sim_pos -0.8645, sim_neg -1.4554, sim_diff 0.5909, time 16.28s\n",
            "iter 38300, loss 0.7365, sim_pos -0.9515, sim_neg -1.6425, sim_diff 0.6909, time 16.24s\n",
            "iter 38400, loss 0.8758, sim_pos -1.1481, sim_neg -1.4075, sim_diff 0.2593, time 16.11s\n",
            "iter 38500, loss 0.6583, sim_pos -1.0735, sim_neg -2.1983, sim_diff 1.1248, time 16.16s\n",
            "iter 38600, loss 0.6032, sim_pos -0.6351, sim_neg -1.6811, sim_diff 1.0460, time 16.38s\n",
            "iter 38700, loss 0.6481, sim_pos -1.1254, sim_neg -2.0610, sim_diff 0.9355, time 16.20s\n",
            "iter 38800, loss 0.6276, sim_pos -1.0125, sim_neg -1.9065, sim_diff 0.8939, time 16.17s\n",
            "iter 38900, loss 0.7018, sim_pos -1.0139, sim_neg -2.0061, sim_diff 0.9921, time 16.19s\n",
            "iter 39000, loss 0.5864, sim_pos -0.8387, sim_neg -1.9553, sim_diff 1.1166, val/pair_auc 0.8454, val/triplet_acc 0.8440, time 23.35s\n",
            "iter 39100, loss 0.9252, sim_pos -1.1772, sim_neg -1.4495, sim_diff 0.2722, time 16.10s\n",
            "iter 39200, loss 0.5433, sim_pos -0.8448, sim_neg -2.0454, sim_diff 1.2005, time 16.31s\n",
            "iter 39300, loss 0.6748, sim_pos -0.8668, sim_neg -1.6930, sim_diff 0.8262, time 16.20s\n",
            "iter 39400, loss 0.6101, sim_pos -0.9188, sim_neg -2.1503, sim_diff 1.2315, time 16.39s\n",
            "iter 39500, loss 0.7803, sim_pos -0.9221, sim_neg -1.6516, sim_diff 0.7295, time 16.18s\n",
            "iter 39600, loss 0.7083, sim_pos -1.0152, sim_neg -1.6352, sim_diff 0.6200, time 16.21s\n",
            "iter 39700, loss 0.5427, sim_pos -0.7180, sim_neg -1.9954, sim_diff 1.2774, time 16.21s\n",
            "iter 39800, loss 0.8738, sim_pos -1.1363, sim_neg -1.5301, sim_diff 0.3938, time 16.32s\n",
            "iter 39900, loss 0.6670, sim_pos -0.9523, sim_neg -1.8457, sim_diff 0.8934, time 16.47s\n",
            "iter 40000, loss 0.7887, sim_pos -1.0414, sim_neg -1.5750, sim_diff 0.5337, val/pair_auc 0.8406, val/triplet_acc 0.8490, time 23.17s\n",
            "iter 40100, loss 0.6118, sim_pos -1.0369, sim_neg -2.2798, sim_diff 1.2429, time 16.15s\n",
            "iter 40200, loss 0.7619, sim_pos -1.0191, sim_neg -1.5905, sim_diff 0.5715, time 16.23s\n",
            "iter 40300, loss 0.7777, sim_pos -1.0674, sim_neg -1.6130, sim_diff 0.5455, time 16.20s\n",
            "iter 40400, loss 0.8159, sim_pos -1.1654, sim_neg -1.8253, sim_diff 0.6599, time 16.36s\n",
            "iter 40500, loss 0.7023, sim_pos -0.9859, sim_neg -1.5813, sim_diff 0.5954, time 16.09s\n",
            "iter 40600, loss 0.6850, sim_pos -0.8001, sim_neg -1.5414, sim_diff 0.7413, time 16.25s\n",
            "iter 40700, loss 0.7233, sim_pos -0.8702, sim_neg -1.6039, sim_diff 0.7338, time 16.22s\n",
            "iter 40800, loss 0.6687, sim_pos -0.7744, sim_neg -1.4700, sim_diff 0.6957, time 16.12s\n",
            "iter 40900, loss 0.7954, sim_pos -1.0855, sim_neg -1.6054, sim_diff 0.5199, time 16.08s\n",
            "iter 41000, loss 0.8194, sim_pos -1.0525, sim_neg -1.4688, sim_diff 0.4163, val/pair_auc 0.8470, val/triplet_acc 0.8400, time 23.22s\n",
            "iter 41100, loss 0.5400, sim_pos -0.7647, sim_neg -1.7881, sim_diff 1.0234, time 16.06s\n",
            "iter 41200, loss 0.6445, sim_pos -0.8563, sim_neg -1.8204, sim_diff 0.9641, time 16.02s\n",
            "iter 41300, loss 0.6887, sim_pos -1.0025, sim_neg -1.6597, sim_diff 0.6572, time 16.19s\n",
            "iter 41400, loss 0.5596, sim_pos -1.0538, sim_neg -2.1890, sim_diff 1.1351, time 16.34s\n",
            "iter 41500, loss 0.7133, sim_pos -0.9823, sim_neg -1.5758, sim_diff 0.5935, time 16.06s\n",
            "iter 41600, loss 0.8013, sim_pos -0.8989, sim_neg -1.3044, sim_diff 0.4055, time 16.03s\n",
            "iter 41700, loss 0.6234, sim_pos -0.9143, sim_neg -1.7420, sim_diff 0.8277, time 16.21s\n",
            "iter 41800, loss 0.6101, sim_pos -0.9542, sim_neg -1.8710, sim_diff 0.9167, time 16.16s\n",
            "iter 41900, loss 0.5721, sim_pos -0.8581, sim_neg -1.9899, sim_diff 1.1318, time 16.32s\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Set GPU\n",
        "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
        "#device = torch.device('cpu')\n",
        "\n",
        "\n",
        "# Print configure\n",
        "config = get_default_config()\n",
        "for (k, v) in config.items():\n",
        "    print(\"%s= %s\" % (k, v))\n",
        "\n",
        "# Set random seed\n",
        "seed = config['seed']\n",
        "random.seed(seed)\n",
        "np.random.seed(seed + 1)\n",
        "torch.manual_seed(seed + 2)\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "config['data']['problem'] = 'graph_edit_distance'\n",
        "# config['data']['problem'] = 'QM7b'\n",
        "config['model_type'] = 'matching'\n",
        "# config['model_type'] = 'embedding'\n",
        "\n",
        "training_set, validation_set = build_datasets(config)\n",
        "\n",
        "if config['training']['mode'] == 'pair':\n",
        "  #默认的是pair，默认的batch_size为20\n",
        "    training_data_iter = training_set.pairs(config['training']['batch_size'])\n",
        "    \n",
        "    \n",
        "    first_batch_graphs, _ = next(training_data_iter)\n",
        "else:\n",
        "    training_data_iter = training_set.triplets(config['training']['batch_size'])\n",
        "    first_batch_graphs = next(training_data_iter)\n",
        "\n",
        "node_feature_dim = first_batch_graphs.node_features.shape[-1]\n",
        "edge_feature_dim = first_batch_graphs.edge_features.shape[-1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model, optimizer = build_model(config, node_feature_dim, edge_feature_dim)\n",
        "model.to(device)\n",
        "\n",
        "accumulated_metrics = collections.defaultdict(list)\n",
        "\n",
        "\n",
        "training_n_graphs_in_batch = config['training']['batch_size']\n",
        "if config['training']['mode'] == 'pair':\n",
        "    training_n_graphs_in_batch *= 2\n",
        "elif config['training']['mode'] == 'triplet':\n",
        "    training_n_graphs_in_batch *= 4\n",
        "else:\n",
        "    raise ValueError('Unknown training mode: %s' % config['training']['mode'])\n",
        "\n",
        "t_start = time.time()\n",
        "for i_iter in range(config['training']['n_training_steps']):\n",
        "    model.train(mode=True)\n",
        "    batch = next(training_data_iter)\n",
        "    if config['training']['mode'] == 'pair':\n",
        "        node_features, edge_features, from_idx, to_idx, graph_idx, labels = get_graph(batch)\n",
        "        \n",
        "        labels = labels.to(device)\n",
        "    else:\n",
        "\n",
        "        node_features, edge_features, from_idx, to_idx, graph_idx = get_graph(batch)\n",
        "\n",
        "    graph_vectors = model(node_features.to(device), edge_features.to(device), from_idx.to(device), to_idx.to(device),graph_idx.to(device), training_n_graphs_in_batch)\n",
        "\n",
        "    if config['training']['mode'] == 'pair':\n",
        "        x, y = reshape_and_split_tensor(graph_vectors, 2)\n",
        "\n",
        "\n",
        "        loss = pairwise_loss(x, y, labels,\n",
        "                             loss_type=config['training']['loss'],\n",
        "                             margin=config['training']['margin'])\n",
        "\n",
        "        is_pos = (labels == torch.ones(labels.shape).long().to(device)).float()\n",
        "        is_neg = 1 - is_pos\n",
        "        n_pos = torch.sum(is_pos)\n",
        "        n_neg = torch.sum(is_neg)\n",
        "\n",
        "\n",
        "\n",
        "        sim = compute_similarity(config, x, y)\n",
        "        sim_pos = torch.sum(sim * is_pos) / (n_pos + 1e-8)\n",
        "        sim_neg = torch.sum(sim * is_neg) / (n_neg + 1e-8)\n",
        "    else:\n",
        "        x_1, y, x_2, z = reshape_and_split_tensor(graph_vectors, 4)\n",
        "        loss = triplet_loss(x_1, y, x_2, z,\n",
        "                            loss_type=config['training']['loss'],\n",
        "                            margin=config['training']['margin'])\n",
        "\n",
        "        sim_pos = torch.mean(compute_similarity(config, x_1, y))\n",
        "        sim_neg = torch.mean(compute_similarity(config, x_2, z))\n",
        "\n",
        "    graph_vec_scale = torch.mean(graph_vectors ** 2)\n",
        "    if config['training']['graph_vec_regularizer_weight'] > 0:\n",
        "        loss += (config['training']['graph_vec_regularizer_weight'] *\n",
        "                 0.5 * graph_vec_scale)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    loss.backward(torch.ones_like(loss))  #只支持pytorch1.2及以上\n",
        "    nn.utils.clip_grad_value_(model.parameters(), config['training']['clip_value'])\n",
        "    optimizer.step()\n",
        "\n",
        "    sim_diff = sim_pos - sim_neg\n",
        "    accumulated_metrics['loss'].append(loss)\n",
        "    accumulated_metrics['sim_pos'].append(sim_pos)\n",
        "    accumulated_metrics['sim_neg'].append(sim_neg)\n",
        "    accumulated_metrics['sim_diff'].append(sim_diff)\n",
        "\n",
        "\n",
        "    # evaluation\n",
        "    if (i_iter + 1) % config['training']['print_after'] == 0:\n",
        "        metrics_to_print = {\n",
        "            k: torch.mean(v[0]) for k, v in accumulated_metrics.items()}\n",
        "        info_str = ', '.join(\n",
        "            ['%s %.4f' % (k, v) for k, v in metrics_to_print.items()])\n",
        "        # reset the metrics\n",
        "        accumulated_metrics = collections.defaultdict(list)\n",
        "\n",
        "        if ((i_iter + 1) // config['training']['print_after'] %\n",
        "                config['training']['eval_after'] == 0):\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                accumulated_pair_auc = []\n",
        "                for batch in validation_set.pairs(config['evaluation']['batch_size']):\n",
        "                    node_features, edge_features, from_idx, to_idx, graph_idx, labels = get_graph(batch)\n",
        "                    labels = labels.to(device)\n",
        "                    eval_pairs = model(node_features.to(device), edge_features.to(device), from_idx.to(device),\n",
        "                                       to_idx.to(device),\n",
        "                                       graph_idx.to(device), config['evaluation']['batch_size'] * 2)\n",
        "\n",
        "                    x, y = reshape_and_split_tensor(eval_pairs, 2)\n",
        "                    similarity = compute_similarity(config, x, y)\n",
        "                    pair_auc = auc(similarity, labels)\n",
        "                    accumulated_pair_auc.append(pair_auc)\n",
        "\n",
        "                accumulated_triplet_acc = []\n",
        "                for batch in validation_set.triplets(config['evaluation']['batch_size']):\n",
        "                    node_features, edge_features, from_idx, to_idx, graph_idx = get_graph(batch)\n",
        "                    eval_triplets = model(node_features.to(device), edge_features.to(device), from_idx.to(device),\n",
        "                                          to_idx.to(device),\n",
        "                                          graph_idx.to(device),\n",
        "                                          config['evaluation']['batch_size'] * 4)\n",
        "                    x_1, y, x_2, z = reshape_and_split_tensor(eval_triplets, 4)\n",
        "                    sim_1 = compute_similarity(config, x_1, y)\n",
        "                    sim_2 = compute_similarity(config, x_2, z)\n",
        "                    triplet_acc = torch.mean((sim_1 > sim_2).float())\n",
        "                    accumulated_triplet_acc.append(triplet_acc.cpu().numpy())\n",
        "\n",
        "                eval_metrics = {\n",
        "                    'pair_auc': np.mean(accumulated_pair_auc),\n",
        "                    'triplet_acc': np.mean(accumulated_triplet_acc)}\n",
        "                info_str += ', ' + ', '.join(\n",
        "                    ['%s %.4f' % ('val/' + k, v) for k, v in eval_metrics.items()])\n",
        "            model.train()\n",
        "        print('iter %d, %s, time %.2fs' % (\n",
        "            i_iter + 1, info_str, time.time() - t_start))\n",
        "        t_start = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUgfceScpDWS"
      },
      "source": [
        "# New Section"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "QM7b.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPD1BA1+kydRbwMi7qLPToU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}